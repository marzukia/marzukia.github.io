[{"content":"","date":null,"permalink":"/","section":"Andryo Marzuki - Net Zero Productivity by 2050","summary":"","title":"Andryo Marzuki - Net Zero Productivity by 2050"},{"content":"","date":null,"permalink":"/tags/caching/","section":"Tags","summary":"","title":"Caching"},{"content":"","date":null,"permalink":"/tags/django/","section":"Tags","summary":"","title":"Django"},{"content":"","date":null,"permalink":"/tags/gis/","section":"Tags","summary":"","title":"GIS"},{"content":"","date":null,"permalink":"/tags/maplibre/","section":"Tags","summary":"","title":"MapLibre"},{"content":"","date":null,"permalink":"/tags/mvt/","section":"Tags","summary":"","title":"MVT"},{"content":"","date":null,"permalink":"/tags/performance/","section":"Tags","summary":"","title":"Performance"},{"content":"If you\u0026rsquo;ve read the title of the post and actually clicked this post I\u0026rsquo;m going to make an assumption that you are already someone who has technical knowledge in this domain, as such, this post will be quite technical and go into some of the most important things I\u0026rsquo;ve learnt over the last eight years.\nModern spatial applications face unique challenges when dealing with large geospatial datasets, real-time mapping interfaces, and complex spatial queries. In other words, it can often be an extreme pain in the ass to get things working smoothly. In retrospect, I\u0026rsquo;ve spent an embarrassing amount of time wrestling with spatial web applications. What started as a small side-project turned into a career, and finally into what I can only describe as a love-hate relationship with anything involving coordinates and polygons.\nThe most frustrating theme I encountered during this journey was just how opaque (or out of date) everything seemed to be. You\u0026rsquo;d think after decades of people building mapping applications, there would be comprehensive guides on how to make them not perform like absolute rubbish. Instead, you get fragments of knowledge scattered across blog posts, Stack Overflow answers, and the occasional conference talk that leaves you with more questions than answers (or worse yet, \u0026ldquo;edit: nvm fixed it\u0026rdquo;).\nThe spatial development community seems to have this unspoken agreement that everyone should figure things out the hard way. However, I\u0026rsquo;ve never been one to gatekeep information, so this post will attempt to document everything I wish someone had told me when I first started building spatial applications that needed to handle more than a dozen points without bringing the browser to its knees.\nPreamble\nMy experience has been using PostGIS as my spatial database and various ORMs as my server, however, most of my examples will show Django examples when showing pseudo/boilerplate code.\nThe database of your web application is incredibly important as it\u0026rsquo;s what you\u0026rsquo;ll want to use as the main backbone of any spatial functionality or activity. For example, if you have the ability to directly serve spatial tables already rendered as GeoJSON or MVT, you should do that. Whilst you can use your application layer to do some manipulation, it will be infinitely slower than leveraging your database.\nFor all of the suggestions/tricks in this post, you\u0026rsquo;ll need to do the mental calculus whether or not the benefits from introducing some of these tweaks is greater than the introduced complexity overhead.\nTL;DR # Index properly: GiST for polygons/complex geometries, SP‑GiST for large clustered points. Use compound indexes when filtering by attributes + geometry. Optimize queries early: use bbox predicates first (\u0026amp;\u0026amp;), avoid unnecessary ST_Transform on indexed columns, and EXPLAIN regularly. Subdivide big geometries: ST_Subdivide during ETL to let the planner prune quickly; keep originals only if you need perfect visuals. Partition large tables: choose keys that match common filters so the planner prunes partitions; keep constraints and indexes tight. Serve MVT from PostGIS: generate tiles in the DB (ST_AsMVTGeom/ST_AsMVT), and cache aggressively. Frontend sanity: use MapLibre “overlay anchors” to control layer order; push heavy serialization to the client only when clustering demands it. Spatial Indexing Strategy #The foundation of any high-performance spatial application lies in proper spatial indexing. Like any index, spatial indexes are way for your database to effectively filter our large volumes of irrelevant rows, it does this by creating a bounding box over a geometry.\nPostGIS provides several indexing options, each suited for different use cases:\n-- Standard GiST spatial index for general geometry queries CREATE INDEX idx_spatial_geom ON spatial_table USING GIST(geom); -- Compound indexes for filtered spatial queries CREATE INDEX idx_spatial_type_geom ON spatial_table USING GIST(category_id, geom); -- SP-GiST for point data with natural clustering CREATE INDEX idx_points_spgist ON point_table USING SPGIST(location); Key Principles:\nUse GiST indexes for polygons and complex geometries Consider SP-GiST for point datasets with natural clustering patterns Create compound indexes when filtering by attributes and geometry together Monitor index usage and rebuild when fragmentation occurs Gotchas:\nDon\u0026rsquo;t assume that a simple geometry can be effectively indexed. If, for example, you have a very large square across the entire country, that index is essentially useless. Watch your projections, if you\u0026rsquo;re using an ST_Transform in a query and the index of the geometry column has been done in the original projection, that spatial index will not be used. When to use / Trade‑offs: Always index geometry columns used for spatial predicates; compound indexes help mixed filters but increase write cost and disk usage.\nGeometry Subdivision for Performance #Large and/or complex geometries can severely impact query performance. One of the most effective techniques I\u0026rsquo;ve discovered is using PostGIS\u0026rsquo;s ST_Subdivide function during your ETL process to break down unwieldy polygons into manageable chunks.\nHowever, it should be important that this is not something you should always do universally. Subdividing geometries adds additional complexity to your application, so when deciding whether or not to subdivide, I always do the mental calculus in determining whether the increased complexity is worth the performance increases.\nIf you reduce this concept its simplest form, you can think of it like this - having a large spatial index is essentially the same as having no index, subdivision enables the query engine to eliminate the majority of a large geometry.\nImplementation Approaches #Practically, I think implementation options of implementing geometry subdivision in your application can boil down to the following approaches:\nUsing ETL process; and/or Using specially structured queries; and/or Using your application layer. All have their own complexity overheads and advantages which I\u0026rsquo;ll talk through in more detail in the next few subsections. Note, you may need to do more than one depending on what you\u0026rsquo;re doing with the data.\nWhen to use / Trade‑offs: Subdivide when geometries are large/complex and queried spatially; it speeds intersects/tiling but adds ETL/storage complexity and requires careful query routing.\nIn some ways, this is generally the easiest option to do if you want a simple implementation of this approach. The idea is that you implement additional database layers where you can transform any of the source spatial data into its subdivided parts before merging into your public layer.\nIf visual representation of spatial data is not required in the frontend or, if you want to handle aggregating/union the geometries in the application layer you may not need to maintain the original geometries. In this scenario, your implementation of the would almost be as simple as the following:\nflowchart LR subgraph db subgraph load load.foobar end subgraph staging staging.foobar end subgraph public public.foobar end end spatial.gpkg -- ogr2ogr --\u003e load.foobar -- st_subdivide --\u003e staging.foobar -- geom --\u003e public.foobar If you want to maintain the original geometries for visual purposes, for data lineage, accuracy, or any number of other reasons, your approach would look something like:\nflowchart LR subgraph db subgraph load load.foobar end subgraph staging staging.foobar end subgraph optimised optimised.foobar_geometries end subgraph public public.foobar end end spatial.gpkg -- ogr2ogr --\u003e load.foobar --\u003e staging.foobar -- st_subdivide --\u003e optimised.foobar_geometries staging.foobar -- geom --\u003e public.foobar This approach is generally more complex as there\u0026rsquo;s more moving parts. You\u0026rsquo;ll need to implement specific application logic or a specific approach in how you query your data to ensure you utilise the subdivided geometries for any expensive operations. Additionally, depending on the size of the dataset, things like storage may need to be taken into account when doing your mental calculus.\nETL Basic Example\n-- Step 1: Load raw data into staging CREATE TABLE staging.foobar AS SELECT id, name, category, geom FROM load.foobar; -- Step 2: Create subdivided table directly CREATE TABLE public.foobar AS SELECT id, name, category, ST_Subdivide(geom, 256) as geom -- 256 vertices max per subdivision FROM staging.foobar WHERE ST_IsValid(geom); -- Step 3: Add spatial index for performance CREATE INDEX idx_foobar_geom ON public.foobar USING GIST (geom); ETL Maintaining Original Geometries Example\n-- Step 1: Staging table (original geometries) CREATE TABLE staging.foobar AS SELECT id, name, category, area_km2, geom as original_geom FROM load.foobar; -- Step 2: Create optimized subdivisions for spatial operations CREATE TABLE optimised.foobar_geometries AS SELECT id, generate_series(1, ST_NumGeometries(subdivided_geom)) as subdivision_id, ST_GeometryN(subdivided_geom, generate_series(1, ST_NumGeometries(subdivided_geom))) as geom FROM ( SELECT id, ST_Collect(ST_Subdivide(original_geom, 256)) as subdivided_geom FROM staging.foobar WHERE ST_IsValid(original_geom) ) subdivisions; -- Step 3: Public table with original geometries for display CREATE TABLE public.foobar AS SELECT id, name, category, area_km2, original_geom as geom -- Keep original for visual accuracy FROM staging.foobar; -- Step 4: Indexes for both tables CREATE INDEX idx_foobar_public_geom ON public.foobar USING GIST (geom); CREATE INDEX idx_foobar_optimised_geom ON optimised.foobar_geometries USING GIST (geom); CREATE INDEX idx_foobar_optimised_id ON optimised.foobar_geometries (id); Query Examples\n-- Use optimised table for expensive spatial operations SELECT DISTINCT f.id, f.name FROM public.foobar f WHERE f.id IN ( SELECT DISTINCT og.id FROM optimised.foobar_geometries og WHERE ST_Intersects(og.geom, ST_GeomFromText(\u0026#39;POINT(144.9631 -37.8136)\u0026#39;, 4326)) ); -- Generate MVT tiles using subdivided geometries for better performance SELECT ST_AsMVT(tile_data, \u0026#39;foobar_layer\u0026#39;) as mvt FROM ( SELECT f.id, f.name, f.category, ST_AsMVTGeom( og.geom, ST_TileEnvelope($1, $2, $3), -- z, x, y parameters 4096, 256 ) as geom FROM optimised.foobar_geometries og JOIN public.foobar f ON f.id = og.id WHERE ST_Intersects( og.geom, ST_Transform(ST_TileEnvelope($1, $2, $3), 4326) ) ) tile_data WHERE geom IS NOT NULL -- Refresh subdivided geometries after data updates TRUNCATE optimised.foobar_geometries; INSERT INTO optimised.foobar_geometries (id, subdivision_id, geom) SELECT id, generate_series(1, ST_NumGeometries(subdivided_geom)) as subdivision_id, ST_GeometryN(subdivided_geom, generate_series(1, ST_NumGeometries(subdivided_geom))) as geom FROM ( SELECT id, ST_Collect(ST_Subdivide(geom, 256)) as subdivided_geom FROM public.foobar WHERE ST_IsValid(geom) AND updated_at \u0026gt; (SELECT COALESCE(MAX(created_at), \u0026#39;1970-01-01\u0026#39;) FROM optimised.foobar_geometries) ) subdivisions; Note: If you\u0026rsquo;re using an ORM like Django, the simplest approach in actually implementing the usage of these subdivided geometries (if stored separately) is to create or override the manager of your data model.\nGotchas:\nYou may need to adjust your subdivision approach and add additional additional parameters in determining subdivision candidates. For example, if you have an extremely large square covering the entire planet, any index will be useless. The optimal number of vertices to subdivide by will depend on the infrastructure you have available. Table Partitioning for Aggregated/Large Datasets #When designing web applications which have multi-tenancy or SaaS-esque, it\u0026rsquo;s important to reduce the amount of \u0026ldquo;configuration by code\u0026rdquo; that\u0026rsquo;s required. Spatial tables in particular are a bit of a pain in things like spatial projections, etc. In the spatial applications I\u0026rsquo;ve built, I generally go for a \u0026ldquo;common\u0026rdquo; or \u0026ldquo;normalised\u0026rdquo; table approach where I destructure spatial tables into a set of tables. This means I can load entirely new datasets into an application without any code changes.\nHowever, this means that optimisation is extremely important as some tables have at times are in the billions of rows. Partitioning is crucial for applications dealing with large volumes of spatial records. However, the specific strategy in how you implement partitioning will be dependent on what your application is doing and the context of your datasets.\nFor example, if you were doing the same approach as I normally do, you would structure your table something like:\n-- Partition by data category for optimized query performance CREATE TABLE spatial_data ( id SERIAL, category_id INTEGER, geom GEOMETRY, properties JSONB, primary key (id, category_id) ) PARTITION BY LIST (category_id); You\u0026rsquo;ll also need a way to create, delete and manage partitions. Assuming usage of an ORM like Django, my approach is to use something like a post_save receiver which watches changes. If you\u0026rsquo;re not using the ORM to load the data and using direct loads e.g. ogr2ogr, psql - you\u0026rsquo;ll need to make sure these partitions are created through migrations (or similar concept).\n# Use database signals for automatic partition creation @receiver(post_save, sender=SpatialData) def ensure_partition_exists(sender, instance, **kwargs): partition_name = f\u0026#34;spatial_data_{instance.category_id}\u0026#34; create_partition_if_not_exists(partition_name, instance.category_id) Gotchas\nIf performance is still poor, make sure you\u0026rsquo;ve got your indexes set up correctly. Partition tables will have multi-column primary indexes generally. Depending on the scale of your data, you\u0026rsquo;ll likely need to add additional partitioning rules such as country of dataset, or even state/locality, etc. When to use / Trade‑offs: Use partitioning when tables are huge or retention/tenancy splits are natural; you’ll gain planner pruning and maintenance wins at the cost of more DDL and index overhead.\nQuery Optimization Techniques #Spatial Query Optimization:\n-- Use bounding box queries before expensive spatial operations SELECT * FROM spatial_table WHERE geom \u0026amp;\u0026amp; ST_MakeEnvelope(xmin, ymin, xmax, ymax, 4326) AND ST_Intersects(geom, target_geometry); -- Leverage spatial relationship hierarchy -- ST_Intersects (fast) -\u0026gt; ST_Contains (medium) -\u0026gt; ST_Within (detailed) Performance Monitoring:\n-- Monitor spatial query performance EXPLAIN (ANALYZE, BUFFERS) SELECT * FROM spatial_table WHERE ST_DWithin(geom, point_geometry, 1000); Vector Tiles for High-Performance Mapping #For a very long time, I was stuck using an old version of Leaflet for the frontend of one of the applications I was developing. This was entirely due to the fact that the application was forced to use a whole load of outdated internal repositories. However, a few years ago I had switched the majority of my projects to using MapLibre and vector tiles were a complete game changer for me\u0026hellip; they are the absolute best.\nInstead of sending structured JSON (or GeoJSON) to your client, you send protobuf vector tiles directly to MapLibre. It essentially copies the same approach that raster tiles use, i.e. using x, y, zoom to limit both data retrieved and fidelity of data based on how your map is positioned and zoomed.\nImplementation Approaches #The below is a high level overview of how I\u0026rsquo;ve generally structured my applications. Depending on the complexity of your needs, you may forgo serving vector tiles from your backend entirely if all you need is basically visualisation purposes. For example, Martin is a pretty awesome library I\u0026rsquo;ve used on some no-server projects.\nHowever, if you do have a server in the mix, your setup will likely end up looking like:\nsequenceDiagram participant Client as React+MapLibre participant API as Django participant Cache as Redis participant DB as PostGIS Client-\u003e\u003eAPI: Request tile /z/x/y.mvt API-\u003e\u003eCache: Check cache key alt Cache Hit Cache--\u003e\u003eAPI: Return cached MVT API--\u003e\u003eClient: Binary MVT data else Cache Miss API-\u003e\u003eDB: Query spatial data DB-\u003e\u003eDB: ST_AsMVT generation DB--\u003e\u003eAPI: Binary MVT API-\u003e\u003eCache: Store with infinite TTL API--\u003e\u003eClient: Binary MVT data end In this scenario, whilst the server/application layer will be serving the vector tiles, we want to almost exclusively use the database to do this operation as it\u0026rsquo;s expensive.\nWhen to use / Trade‑offs: Serve MVT for dynamic, interactive maps at scale; DB‑generated tiles are fast and cacheable but shift CPU to the database—monitor load and cache aggressively.\nRouting #I generally like to structure my endpoints in this type of pattern:\nGET /api/tiles/{layer}/{z}/{x}/{y}.mvt GET /api/tiles/{layer}/{z}/{x}/{y}.mvt?filter={encoded_filter} In the case of Django my implementation approach would be to create a selector that executes my MVT directly into an HTTP response, here\u0026rsquo;s a generic boilerplate I created for myself when scaffolding new projects.\n@router.get(\u0026#34;/layer/{zoom}/{x}/{y}\u0026#34;, response=bytes) def my_tile_view(request, zoom, x, y): queryset = SpatialModel.objects.filter(active=True) return spatial_queryset_to_tile( queryset=queryset, zoom=zoom, x=x, y=y, geometry_field=\u0026#39;geometry\u0026#39;, property_fields=[\u0026#39;name\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;value\u0026#39;], layer_name=\u0026#39;my_layer\u0026#39; ) Helpers #My preference in serving MVT using an ORM like Django is to directly serve the output of the PostGIS function as a response.\nfrom typing import List, NamedTuple, Optional from django.db import connection, models from django.http import HttpResponse class TileCoordinate(NamedTuple): zoom: int x: int y: int def generate_tile_envelope(tile: TileCoordinate) -\u0026gt; str: return f\u0026#34;ST_SetSRID(ST_TileEnvelope({tile.zoom}, {tile.x}, {tile.y}), 3857)\u0026#34; def queryset_to_mvt_response( queryset: models.QuerySet, tile: TileCoordinate, geometry_field: str = \u0026#34;geom\u0026#34;, property_fields: Optional[List[str]] = None, layer_name: str = \u0026#34;default_layer\u0026#34;, source_srid: int = 4326, ) -\u0026gt; HttpResponse: \u0026#34;\u0026#34;\u0026#34; Convert Django QuerySet to Mapbox Vector Tile HTTP response. Args: queryset: Django QuerySet containing spatial data tile: Tile coordinate object geometry_field: Name of geometry field in model property_fields: List of fields to include as properties (auto-detected if None) layer_name: Name for the MVT layer source_srid: Source SRID of geometry data Returns: HttpResponse containing MVT binary data with appropriate headers \u0026#34;\u0026#34;\u0026#34; # Auto-detect fields if not provided if property_fields is None: property_fields = [] model = queryset.model for field in model._meta.get_fields(): if ( field.concrete and not field.is_relation and not field.one_to_one and not (field.many_to_one and field.related_model) and getattr(field, \u0026#34;column\u0026#34;, field.name) != geometry_field ): property_fields.append(getattr(field, \u0026#34;column\u0026#34;, field.name)) # Generate base SQL from QuerySet base_query, query_params = queryset.query.get_compiler(\u0026#34;default\u0026#34;).as_sql() # Generate tile envelope tile_envelope = generate_tile_envelope(tile) # Build MVT SQL query property_columns = \u0026#34;, \u0026#34;.join(f\u0026#34;t.{field}\u0026#34; for field in property_fields) mvt_sql = f\u0026#34;\u0026#34;\u0026#34; WITH tile_bounds AS ( SELECT {tile_envelope} AS envelope, {tile_envelope}::box2d AS bbox ), tile_data AS ( SELECT ST_AsMVTGeom( ST_Transform(t.{geometry_field}, 3857), tile_bounds.bbox, 4096, -- tile extent 256, -- buffer pixels true -- clip geometry ) AS geom, {property_columns} FROM ({base_query}) t CROSS JOIN tile_bounds WHERE ST_Intersects( ST_Transform(t.{geometry_field}, 3857), tile_bounds.envelope ) ) SELECT ST_AsMVT(tile_data.*, \u0026#39;{layer_name}\u0026#39;, 4096) FROM tile_data WHERE geom IS NOT NULL; \u0026#34;\u0026#34;\u0026#34; # Execute query and get MVT binary data with connection.cursor() as cursor: cursor.execute(mvt_sql, query_params) result = cursor.fetchone() mvt_data = result[0] if result and result[0] else b\u0026#34;\u0026#34; # Create HTTP response with appropriate headers response = HttpResponse(mvt_data, content_type=\u0026#34;application/x-protobuf\u0026#34;) response[\u0026#34;Content-Disposition\u0026#34;] = ( f\u0026#39;attachment; filename=\u0026#34;tile_{tile.zoom}_{tile.x}_{tile.y}.mvt\u0026#34;\u0026#39; ) response[\u0026#34;Content-Encoding\u0026#34;] = \u0026#34;gzip\u0026#34; response[\u0026#34;Cache-Control\u0026#34;] = \u0026#34;public, max-age=3600\u0026#34; # Cache for 1 hour return response # Convenience function for common use cases def spatial_queryset_to_tile( queryset: models.QuerySet, zoom: int, x: int, y: int, **kwargs ) -\u0026gt; HttpResponse: \u0026#34;\u0026#34;\u0026#34; Convenience wrapper for generating MVT tiles from spatial QuerySets. Args: queryset: Django QuerySet with spatial data zoom: Tile zoom level x: Tile x coordinate y: Tile y coordinate **kwargs: Additional arguments passed to queryset_to_mvt_response Returns: HttpResponse with MVT data \u0026#34;\u0026#34;\u0026#34; tile = TileCoordinate(zoom=zoom, x=x, y=y) return queryset_to_mvt_response(queryset, tile, **kwargs) Gotchas\nIt\u0026rsquo;s important when using an ORM like Django you do not just execute raw SQL without using the ORM\u0026rsquo;s connection class. In this case, I know that Django sanitises any inputs meaning that vulnerabilities like SQL injection are not possible. While this is not entirely related to MVTs, the amount of time I\u0026rsquo;ve been able to execute SQL injection of spatial apps to grab the data I\u0026rsquo;m after is pretty shocking. Where possible, I always want to use the ORM to generate the base query. This means I can leverage all normal QuerySet and Manager functions prior to executing the query. This is useful where you have things like filters, or require calculations involving one or more tables. Point Clustering for Large Datasets #One of my current bug bears with MapLibre is the fact that MVT layers currently lack built-in clustering support. When dealing with massive point datasets (\u0026gt;1M points) which require visualisation and clustering, we are sadly not able to use vector tiles.\nIn these cases, we still need to fall back to JSON/GeoJSON to enable clustering functionality. To do this we essentially have to do everything we can to minimise the time it takes to serialise the data, and the time it takes for the payload to reach the client.\nThere\u0026rsquo;s a finite limit of how far we can optimise in this case before we hit diminishing (or negative) returns. My general approach in dealing with this type of scenario is as follows:\nEnable compression (e.g. gzip) middleware Minimise the data being sent to the frontend by: Sending only the required data in an array of tuples; and/or Removing whitespace; and/or Reducing precision of coordinates; then Utilise a cache if dataset is not dynamic/variable; then Move serialisation/structuring of data to frontend library like @turf to structure the tuples into a valid FeatureCollection I\u0026rsquo;ve been able to reduce payloads by nearly 90% doing the above changes cutting payload transfer time from 40s to under a few seconds.\nTangent: As an offtopic comment, while I was still stuck using Leaflet I had implemented server-side clustering by creating a ladder of zoom grids. However, the complexity this added was significant and was generally inferior to what\u0026rsquo;s available out the box with MapLibre. For those reasons I\u0026rsquo;m not going to give advice on that front here.\nHere\u0026rsquo;s a boilerplate example I\u0026rsquo;ve generalised from one of my personal projects.\n@router.get(\u0026#39;/spatial-layer\u0026#39;) def spatial_layer_endpoint( queryset: QuerySet, geometry_field: str = \u0026#34;geom\u0026#34;, snap_precision: float = 0.00001, coordinate_precision: int = 5, ) -\u0026gt; List[Tuple[int, float, float]]: \u0026#34;\u0026#34;\u0026#34; Generic spatial layer endpoint that returns optimized coordinate data. Args: queryset: Base QuerySet to filter and process geometry_field: Name of geometry field in the model snap_precision: ST_SnapToGrid precision (affects accuracy vs performance) coordinate_precision: Number of decimal places for lat/lng rounding Returns: List of tuples: (id, longitude, latitude) \u0026#34;\u0026#34;\u0026#34; # Apply spatial optimizations optimized_queryset = ( queryset .annotate(snap_geom=SnapToGrid(geometry_field, snap_precision)) .annotate( lat=Round(GeomLat(\u0026#34;snap_geom\u0026#34;), coordinate_precision), lng=Round(GeomLng(\u0026#34;snap_geom\u0026#34;), coordinate_precision), ) .values_list(\u0026#34;id\u0026#34;, \u0026#34;lng\u0026#34;, \u0026#34;lat\u0026#34;) ) # Convert to list for JSON serialization # Note: values_list is used instead of values() to minimize payload return list(optimized_queryset) Gotchas\nAvoid using ORM serializers, they will significantly slow things down. Whilst you can optimise the payload to the browser into a small size (e.g. 4mb), you\u0026rsquo;ll likely run into memory issues with the browser itself. This becomes quite common when you exceed a million points; it\u0026rsquo;s very browser dependent on how it handles memory. If you\u0026rsquo;re using persisted states in your frontend, you need to be careful how data is being stored in the client state (especially ith large amount of records). When to use / Trade‑offs: Use tuple streams + client clustering when you truly need clustering today; payloads shrink, but the client pays in memory/CPU—watch browser limits.\nImplementing Response Caching #Spatial queries are slow and expensive, even with all the optimisations in the world some queries can be extremely slow especially if the query is a complex one.\nCaching is an easy boon to performance if you can justify the additional complexity overhead. In many of my use cases, spatial data is generally slow to update (if ever), and in instances where I need to purely visualise the data, caching is a useful tool as it means that a call that would\u0026rsquo;ve otherwise hit the database is now being served through memory. In this particular scenario, I am only caching static-ish spatial data with triggers to invalidate the cache on any ETL change.\nMy general approach is to create a basic route decorator which determines whether to hit the cache or database:\ndef cache_with_infinite_ttl(func): \u0026#34;\u0026#34;\u0026#34;Cache static geospatial data indefinitely\u0026#34;\u0026#34;\u0026#34; @wraps(func) def wrapper(*args, **kwargs) -\u0026gt; Any: cache_key: str = f\u0026#34;static:{func.__name__}:{hash(str(args) + str(kwargs))}\u0026#34; result: Optional[Any] = cache.get(cache_key) if result is None: result = func(*args, **kwargs) cache.set(cache_key, result) # No expiration return result return wrapper Then simply wrap my endpoint with the cache decorator:\n@cache_with_infinite_ttl # Static geospatial data def get_static_vector_tile(zoom: int, x: int, y: int, layer_type: str) -\u0026gt; bytes: return generate_tile(zoom, x, y, layer_type) My preference is to have a common prefix which I can use to easily do a bulk cache invalidation if required.\nStatic data: \u0026#34;mvt:{layer}:{zoom}:{x}:{y}\u0026#34; Dynamic data: \u0026#34;mvt:{layer}:{zoom}:{x}:{y}:{filter_hash}\u0026#34; Gotchas\nWith an additional cache like redis implemented, you will essentially have multiple layers of caching. It\u0026rsquo;s important you understand how you will manage invalidation of response across your application. graph TB subgraph \"Client Layer\" A[Browser Cache] B[Service Worker Cache] end subgraph \"CDN Layer\" C[CloudFront/CDN] end subgraph \"Application Layer\" D[Redis Cache] E[Application Memory Cache] end subgraph \"Database Layer\" F[PostGIS Buffer Cache] G[OS File System Cache] end A --\u003e C B --\u003e C C --\u003e D D --\u003e E E --\u003e F F --\u003e G When to use / Trade‑offs: Cache static‑ish tiles and layer responses; invalidation strategy is the hard part—plan keys and triggers upfront.\nForcing React MapLibre Order #Another quirk of MapLibre which drives me somewhat insane is the race conditions that often occur with how it loads its layers; this is specifically an issue when using react-map-gl. Because of how React works, it can quickly get hacky and messy when you start using the map\u0026rsquo;s ref directly to trigger reordering of layers.\nA hack that I\u0026rsquo;ve came up with is a concept called an \u0026lsquo;overlay anchor\u0026rsquo;. Basically, you create empty layers (as many as you need) which act as anchors for the layers being loaded. This means you can safely use beforeId without worrying about race conditions.\nimport { Source, Layer } from \u0026#39;react-map-gl/maplibre\u0026#39; export default function OverlayAnchor() { return ( \u0026lt;Source id=\u0026#34;__overlay-anchor__\u0026#34; type=\u0026#34;geojson\u0026#34; data={{ type: \u0026#39;FeatureCollection\u0026#39;, features: [] }} \u0026gt; \u0026lt;Layer id=\u0026#34;__overlay-top__\u0026#34; type=\u0026#34;symbol\u0026#34; source=\u0026#34;__overlay-anchor__\u0026#34; layout={{ visibility: \u0026#39;none\u0026#39; }} /\u0026gt; \u0026lt;Layer id=\u0026#34;__overlay-second__\u0026#34; type=\u0026#34;symbol\u0026#34; beforeId=\u0026#34;__overlay-top__\u0026#34; source=\u0026#34;__overlay-anchor__\u0026#34; layout={{ visibility: \u0026#39;none\u0026#39; }} /\u0026gt; \u0026lt;Layer id=\u0026#34;__overlay-third__\u0026#34; type=\u0026#34;symbol\u0026#34; beforeId=\u0026#34;__overlay-second__\u0026#34; source=\u0026#34;__overlay-anchor__\u0026#34; layout={{ visibility: \u0026#39;none\u0026#39; }} /\u0026gt; \u0026lt;/Source\u0026gt; ) } Conclusion #Building high-performance spatial applications requires a holistic approach that combines database optimisation, efficient data formats, intelligent caching, and thoughtful architecture decisions. After eight years of making every mistake possible, I can confidently say that implementing these strategies will save you from the pain I\u0026rsquo;ve endured.\nThe key to success lies in understanding your specific use case, measuring performance continuously, and optimising iteratively. Don\u0026rsquo;t try to implement everything at once - you\u0026rsquo;ll drive yourself mad. Start with the foundations (proper indexing and partitioning), then layer on vector tiles, caching, and progressive loading techniques to create truly optimised spatial experiences.\nRemember that spatial application performance isn\u0026rsquo;t just about raw speed, but also about providing smooth, responsive user experiences that scale gracefully as your data and user base grow. And if you\u0026rsquo;ve made it this far through my rambling, you\u0026rsquo;re probably the type of person who cares enough about performance to actually implement these suggestions.\nThe spatial web development community needs more people sharing their hard-won knowledge. If this guide helps you avoid even half the headaches I\u0026rsquo;ve encountered, then the time spent writing it was worthwhile. Now go forth and build something cool (and performant).\n","date":"28 September 2025","permalink":"/posts/building-high-performance-spatial-apps/","section":"Posts","summary":"\u003cp\u003eIf you\u0026rsquo;ve read the title of the post and actually clicked this post I\u0026rsquo;m going to make an assumption that you are already someone who has technical knowledge in this domain, as such, this post will be quite technical and go into some of the most important things I\u0026rsquo;ve learnt over the last eight years.\u003c/p\u003e","title":"Performant Spatial Apps with PostGIS: 8 Years of Head‑Banging"},{"content":"","date":null,"permalink":"/tags/postgis/","section":"Tags","summary":"","title":"PostGIS"},{"content":"","date":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"","date":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python"},{"content":"","date":null,"permalink":"/tags/spatial-indexing/","section":"Tags","summary":"","title":"Spatial Indexing"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":null,"permalink":"/tags/vector-tiles/","section":"Tags","summary":"","title":"Vector Tiles"},{"content":"","date":null,"permalink":"/tags/covid-19/","section":"Tags","summary":"","title":"COVID-19"},{"content":"","date":null,"permalink":"/tags/data-analysis/","section":"Tags","summary":"","title":"Data Analysis"},{"content":"As a non-American observing America’s enthusiastic self-destructing, I find it utterly baffling that a man like Donald Trump could secure a second term. Over half the nation saw the individual who somehow achieved the rare feat of bankrupting not one but two casinos, which are famously designed to print money, as the best choice to lead the country. There\u0026rsquo;s definitely something wrong with America right?\nI\u0026rsquo;ve long entertained a personal pet conspiracy theory that COVID-19 has had a far deeper impact on Trump\u0026rsquo;s re-election in 2024 than most realise. While numerous factors undoubtedly contributed to Trump\u0026rsquo;s victory, this article specifically explores the subtle but significant role COVID-19 may have played.\nMy hypothesis revolves around the idea that COVID-19 exacerbated existing socioeconomic inequalities among marginalised communities, making them easier targets for disenfranchisement, whether intentional or accidental. This was compounded by targeted misinformation campaigns and systemic neglect, ultimately diminishing voter turnout among communities less likely to support Trump.\nAlthough I initially approached this topic somewhat facetiously, the data aligned surprisingly well with my suspicion, compelling me to fully commit to this analysis.\nIn short: Did COVID-19 help Trump win the 2024 election? Probably. But you\u0026rsquo;ll need to read on for the long answer.\nThe Neoliberal Playbook: Disenfranchisement #To understand how COVID-19 may have materially influenced the outcome of the 2024 election, it is first necessary to grasp the role of neoliberalism in shaping contemporary American governance. This section serves as essential context for the remainder of the analysis.\nNeoliberalism in the United States often manifests through policies that systematically disenfranchise those least likely to support conservative interests, particularly marginalised or economically vulnerable communities. This disenfranchisement is not incidental but is integral to the neoliberal project.\nIt typically takes the form of privatising essential public services such as healthcare and education, implementing austerity measures, concentrating decision-making in the hands of unelected technocrats (cough, Elon Musk), and deploying voter suppression tactics including gerrymandering and restrictive voter ID laws. These mechanisms, whether legal, procedural or administrative, function to erect barriers that disproportionately hinder political participation among already disadvantaged groups.\nUnderstanding this deliberate strategy of exclusion is crucial; without it, the broader implications of pandemic-era policies on democratic participation cannot be fully appreciated.\nUndermining Education, Shaping Votes #Given the stark differences in voting patterns between college-educated and non-degree holders, it\u0026rsquo;s unsurprising that Republicans consistently seek to defund public education1. For instance, the 2024 Republican education bill proposed slashing $64 billion (28%) from education budgets, severely impacting critical programmes such as Title I funding, special education, and support for English learners.\nThis deliberate undermining of education cultivates a populace less capable of critically evaluating political information, leaving them more susceptible to emotional manipulation, political apathy, and anti-government sentiment. Conversely, research consistently demonstrates that improved media literacy education significantly boosts civic engagement and reduces political apathy2.\nStatistically, lower education levels, particularly among White voters, correlate strongly with increased Republican support. This dynamic reinforces existing inequalities and pushes marginalised communities further towards political disengagement or manipulation3.\nThe relationship between education, race, and political alignment is stark: Non-White, college-educated voters overwhelmingly favour Democrats, highlighting education\u0026rsquo;s strong correlation with Democratic support. In contrast, White voters without college degrees predominantly vote Republican, illustrating how education intersects significantly with racial identity to shape voting behaviours. This pronounced educational gap remains consistent across racial groups, underscoring education\u0026rsquo;s pivotal role in shaping electoral outcomes.\nYounger voters and voters of colour have increasingly become targets of Republican voter-suppression tactics. In North Carolina, Republicans contested over 65,000 ballots, primarily from college students, unsuccessfully attempting to overturn election results4. Similarly, in Florida, Governor DeSantis\u0026rsquo;s administration penalised voter-registration groups for minor procedural infractions, disproportionately impacting minority communities5.\nMaking Voting Harder (For Some) #Rather than explicitly prohibiting voting, Republicans typically create logistical and administrative hurdles designed to discourage participation among voters unlikely to support them, effectively eroding democracy by attrition.\nBlack voters overwhelmingly support Democrats, and Latino voters also lean Democratic, although less decisively. Conversely, White men strongly favour Republicans, with White women also generally leaning Republican, albeit more moderately. Additionally, women across all racial groups consistently prefer Democrats, emphasising gender\u0026rsquo;s significant influence on voting behaviour.\nA particularly egregious example of Republican tactics is the SAVE Act. Presented as a measure to enhance election security, the Act demands documentary proof of citizenship, such as birth certificates or passports, in order to vote. This significantly disadvantages women who have changed their surnames following marriage, as well as transgender individuals whose documents might not reflect their current identity.\nApproximately 69 million American women lack identification matching their birth names6, and more than half the population does not possess a passport7. Consequently, voting becomes effectively paywalled, disproportionately impacting groups historically less supportive of Republicans.\nRepublican-led voter roll purges, justified by claims of preventing fraud, frequently remove thousands of legitimate voters. For instance, in 2024, Virginia Governor Youngkin’s administration purged over 6,000 voters, many of whom were later confirmed eligible. Despite initial legal challenges, the U.S. Supreme Court ultimately permitted this purge to proceed8. Similar attempts in Alabama, however, were blocked for violating federal election laws9.\nCOVID-19 and the 2024 Election #COVID-19 became a potent instrument of voter disenfranchisement, largely through the proliferation of disinformation campaigns10. Social media platforms were inundated with misleading content, much of it specifically targeting communities of colour, sowing confusion around voting procedures and eligibility.\nThis phenomenon was not confined to the United States. In New Zealand, a similar dynamic unfolded when the rural protest movement Groundswell, originally formed to oppose regulatory and environmental reforms affecting farmers, began attracting support from individuals aligned with anti-lockdown and anti-vaccine ideologies. While these views were not part of Groundswell’s founding purpose, they increasingly found a platform within its broader protest activity, particularly during the height of pandemic-related restrictions.\nAt the time, the Labour Government had implemented stringent but effective lockdown measures. Groundswell received vocal support from the National and ACT parties 11, and later also from New Zealand First. These three parties would go on to form the coalition government following the general election.\nThus, COVID-19 intensified existing inequalities, erected new voting barriers, and amplified targeted misinformation, significantly suppressing democratic participation in marginalised communities.\nVaccinations \u0026amp; Voting #Political affiliation strongly influenced vaccination rates, with clear party divides: 92% Democrats, 68% Independents, and only 56% Republicans vaccinated12.\nA distinct correlation emerged in 2024 data showing lower vaccination rates strongly associated with higher Republican vote shares. This relationship was notably absent in 2020, suggesting vaccination status became a politically defining factor between 2020 and 2024.\nInterestingly, poverty levels did not correlate strongly with lower vaccination rates, countering the intuitive expectation that poorer counties would be less vaccinated due to education or resource barriers.\nEstimating Long COVID\u0026rsquo;s Hidden Impact #This section ventures into more speculative territory. Data constraints limit firm conclusions, but my conspiracy theory general hypothesis includes:\nRepublican-leaning counties, with lower vaccination rates, likely faced greater vulnerability to long COVID. Studies indicate vaccinated individuals experience significantly lower risks of developing persistent symptoms13. Long COVID symptoms (e.g., cognitive impairment or \u0026ldquo;brain fog\u0026rdquo;) could further diminish political engagement, increase susceptibility to misinformation, and exacerbate socioeconomic disparities. Due to limited county-level long COVID data, I\u0026rsquo;ve simulated potential prevalence using COVID case and vaccination rates. This simulation provides a structured estimate of how long COVID may disproportionately impact areas with low vaccine uptake.\nUsing Monte Carlo simulation, I estimated county-level long COVID prevalence based on COVID cases, vaccination status, and established risk factors. Running 10,000 simulations, outcomes were normalised per 100,000 residents and classified by political leaning. Results allowed comparison between Democrat and Republican counties, exploring how vaccination rates potentially influence broader political dynamics.\nRepublican-leaning counties exhibit distinctly higher estimated long COVID prevalence, suggesting a potential public health and socioeconomic divergence along political lines.\nAlthough these estimates can\u0026rsquo;t definitively prove long COVID’s electoral influence, they indicate it likely had some impact. Across counties analysed, around 500,000 long COVID cases were estimated, alongside a notable increase in Republican votes (approximately 2.5 million) between elections, despite incomplete data coverage.\nConclusion #A fundamental rule of statistics remains: correlation does not equal causation. Nonetheless, the evidence strongly suggests COVID-19 played a substantial role in Trump\u0026rsquo;s 2024 victory.\nTo summarise clearly:\nCOVID-19 likely contributed, intentionally or not, to voter disenfranchisement. Low vaccination rates correlated strongly with Republican support in 2024, suggesting vaccination status became politicised. Republican counties, due to lower vaccinations, face higher long COVID risks and potential ongoing cognitive impacts. Political affiliation is more strongly linked with vaccination rates than poverty or education, although these may indirectly influence the situation. If you disagree with my conclusions, please challenge them. While I believe my analysis holds water, constructive discourse is always welcome, and there is always a chance I have overlooked something crucial.\nIn short, COVID-19 just gave us one more reason to dislike it.\nAppendix # GitHub Repo Exit Poll Demographics 2020 \u0026amp; 2024 Voting Data County Level Vaccination Data County Level COVID-19 Data County Poverty Data References # democrats-appropriations.house.gov \u0026ldquo;FACT SHEET: House Republican Funding Bill Denies Education and Training Opportunities for Students and Job Seekers at All Stages of Life\u0026rdquo;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nwww.researchgate.net \u0026ldquo;Media Literacy Dimension in Reinforcing Political Participation Integrity Among Young People in Social Media\u0026rdquo;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nfiles.eric.ed.gov \u0026ldquo;News Media Literacy and Political Engagement: What\u0026rsquo;s the Connection?\u0026rdquo;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nwww.teenvogue.com \u0026ldquo;North Carolina Republicans are Trying to Throw Out College Students’ Votes to Steal an Election\u0026rdquo;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nwww.theguardian.com \u0026ldquo;Revealed: Florida Republicans target voter registration groups with thousands in fines\u0026rdquo;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nwww.americanprogress.org \u0026ldquo;The SAVE Act Would Disenfranchise Millions of Citizens\u0026rdquo;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\ntoday.yougov.com \u0026ldquo;Adults under 30 are more likely than older Americans to have a current U.S. passport\u0026rdquo;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\napnews.com \u0026ldquo;North Carolina judges block GOP law to strip governor’s election board powers\u0026rdquo;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nedition.cnn.com \u0026ldquo;Justice Department sues Alabama over its effort to remove more than 3,000 names from voter rolls too close to election\u0026rdquo;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\ntheemancipator.org \u0026ldquo;Voter suppression 2.0: How digital misinformation targets marginalized communities\u0026rdquo;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nwww.act.org.nz \u0026ldquo;ACT MPs back rural New Zealand at Groundswell\u0026rdquo;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nwww.brookings.edu \u0026ldquo;For COVID-19 vaccinations, party affiliation matters more than race and ethnicity\u0026rdquo;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\npmc.ncbi.nlm.nih.gov \u0026ldquo;Association between long COVID and vaccination: A 12-month follow-up study in a low- to middle-income country\u0026rdquo;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"23 April 2025","permalink":"/posts/did-covid19-make-trump-president/","section":"Posts","summary":"\u003cp\u003eAs a non-American observing America’s enthusiastic self-destructing, I find it utterly baffling that a man like Donald Trump could secure a second term. Over half the nation saw the individual who somehow achieved the rare feat of bankrupting not one but two casinos, which are famously designed to print money, as the best choice to lead the country. There\u0026rsquo;s definitely something wrong with America right?\u003c/p\u003e","title":"Did COVID-19 Help Trump Win In The 2024 Elections? Probably."},{"content":"","date":null,"permalink":"/tags/elections/","section":"Tags","summary":"","title":"Elections"},{"content":"","date":null,"permalink":"/tags/politics/","section":"Tags","summary":"","title":"Politics"},{"content":"","date":null,"permalink":"/tags/usa/","section":"Tags","summary":"","title":"USA"},{"content":"When I relocated to Melbourne last September, I found myself renting again for the first time in nearly a decade. One of the hardest parts of moving to a new country was having no intuitive sense of which neighbourhoods were considered good, up-and-coming, or best avoided.\nI\u0026rsquo;ve now gone unconditional on a property, and it would feel like a waste not to share the data I collected and the analysis I carried out—especially if it can help others navigating the same journey.\nWhere the Hell Do I Buy? #Even with helpful advice from colleagues, I found it difficult to make sense of it all—lacking the local knowledge to properly weigh their suggestions.\nMelbourne proved to be a completely different experience from Auckland. The sheer scale and density of the city meant that just a few kilometres could translate to a 30-minute drive. While public transport here is significantly better than Auckland’s (which, to be fair, is almost non-existent), it has its own quirks.\nTravel to and from the city centre is relatively straightforward, but commuting between nearby suburbs—especially those not directly connected to the CBD—can be surprisingly time-consuming. For example, cycling from Ascot Vale to Carlton North takes about 25 minutes, but the same trip by tram or train can take nearly an hour.\nAmid the usual uncertainty of renting, I felt a growing need to find a more permanent home—somewhere that could offer my daughter a greater sense of long-term stability. I wanted to take a methodical approach, using a simple scoring system to evaluate properties at a glance and filter them from \u0026lsquo;interesting\u0026rsquo; to both interesting and viable.\nFinding \u0026amp; Getting the Data #That’s where I hit the first hurdle: while the data existed, it was often fragmented or hard to access. I had to do a fair bit of tinkering and reverse engineering to extract and consolidate it from various sources.\nThe goal was to build a cohesive view that brought together key livability and accessibility factors.\nNotes # All transport and cycling times are calculated to the ANZ Centre in Docklands and include walking time. If you work elsewhere, your mileage may vary—and some adjustments may be necessary. High-resolution images and source data are available in the appendix. Train \u0026amp; Tram Routes #The data was sourced from Public Transport Victoria’s interactive tools, which don’t offer bulk downloads but are relatively easy to reverse engineer for extracting spatial data. This included tram routes and stops, as well as train lines and station locations.\nKey takeaway: You\u0026rsquo;ll want to be close to a train station or tram stop.\nTransit \u0026amp; Cycling Times to Docklands #This one was straightforward—Google is more than happy to provide this information for a fee.\nTo estimate realistic travel times, I generated a grid of approximately 15,000 sample points spaced at ~0.05 degrees across metropolitan Melbourne. Using the Google Routing API, I calculated public transport and cycling times from each point, targeting an arrival time of 8:30 AM on a Monday.\nPublic Transport #Unsurprisingly, shorter travel times correlate strongly with proximity to train and tram stations. Efficient routes matter more than geographic closeness. Suburbs like Williamstown and St Kilda—while not particularly central—benefit from direct, fast train lines.\nKey takeaway: Proximity to train or tram stations matters more than distance—efficient transport beats geographic closeness.\nCycling #While Google Maps isn’t perfect at modelling cycling routes—particularly when it comes to elevation or actual bike lanes—it was accurate enough to give a rough sense of travel times.\nUnlike public transport, cycling times behave predictably: the further you are, the longer it takes.\nKey takeaway: Distance matters—no shortcuts on a bike.\nPublic School Zoning \u0026amp; Performance #Private schooling is far more common in Melbourne than I’m used to. However, since private schools don’t follow catchment zones, I excluded them and focused solely on public schools.\nZoning data was easy to access through Find My School. Property sites like domain.com.au likely use the same or similar datasets.\nThe more difficult task was assessing school performance. Most available rankings skew heavily toward private and Catholic schools. Fortunately, VCE performance data is publicly available, which allowed me to clean and match public school outcomes to their zones.\nKey takeaway: Better-performing public schools tend to be in the eastern suburbs.\nSuburb Affluence \u0026amp; Gentrification #As I settled into Melbourne, I kept hearing stories of suburbs that had become “nice” over the last decade—classic gentrification. The problem was, I had no personal context to assess these claims. I didn’t know what these places used to be like.\nThis is where census data from the ABS came in handy. While not available in bulk at the postcode (POA) level, summary data was readily accessible through their portal and simple enough to scrape.\nMedian Household Income #The census data confirmed what many locals had told me: wealth is largely concentrated in the eastern suburbs. I didn’t scrape real estate listings, but casual observation suggests that higher income aligns with higher property prices. For scoring purposes, that made these areas less attractive—they were simply too competitive.\nThe ideal is finding suburbs that weren’t historically affluent but have shown clear upward trends—strong signals of ongoing gentrification.\nKey takeaway: Affluence is concentrated in the east, and it often comes with a premium.\nGentrification (2016–2021) #I focused on changes in median monthly household income between the 2016 and 2021 censuses.\nAs expected, already-affluent suburbs showed little movement. But inner-north and inner-west suburbs—like Yarraville, Kingsville, West Footscray, and Brunswick—saw noticeable income growth. The data backed up what I was hearing: these were the suburbs on the rise.\nThis also suggests potential for value—areas that have gentrified but where the market hasn’t fully caught up.\nKey takeaway: The inner north and west have gentrified heavily—there’s still value to be found.\nThe Results #Once all the data was wrangled, I needed a way to turn it into something practical. I built a weighted scoring model to rank suburbs by livability and long-term potential.\nEach postcode was scored across five factors:\nCycle access – ease of cycling into the city Public transport – average travel time to Docklands School quality – based on public school performance Median income – a proxy for affluence Gentrification – change in income over time All metrics were normalised to a 0–1 scale, where lower is better. I then weighted each factor based on its importance to me:\nPublic transport (40%) – the most important Cycling and schools (20% each) – still key, but secondary Income + gentrification (20% combined) – a signal of growth, but also cost The result was a cumulative score that helped highlight suburbs with good fundamentals that still represented value.\nNotes # The weights reflect my priorities—they might differ from yours. The lower the cumulative score, the better. All raw data and scripts are available in the appendix. Top 20 Postcodes # Postcode Cycle Transit School Income Gentrification Cumulative 3054 0.2500 0.3333 0.2576 0.4111 0.4893 0.41492 3053 0.2500 0.2500 0.3835 0.5198 0.4515 0.42096 3002 0.1667 0.2500 0.5909 0.4325 0.5460 0.44722 3065 0.2500 0.2500 0.6790 0.3231 0.4891 0.44824 3052 0.2222 0.3750 0.3352 0.6254 0.3985 0.46626 3206 0.3056 0.3095 0.1761 0.3249 0.9157 0.46826 3000 0.1667 0.1667 0.5890 0.6458 0.6390 0.47478 3056 0.4167 0.4524 0.1108 0.4559 0.4930 0.47624 3011 0.3333 0.4074 0.2500 0.5057 0.4891 0.47858 3066 0.2500 0.3333 0.5909 0.2139 0.6756 0.47940 3057 0.4167 0.5000 0.2481 0.3131 0.4674 0.48906 3205 0.1667 0.3000 0.4091 0.3429 0.9323 0.49020 3182 0.3611 0.3571 0.1364 0.3966 0.8531 0.49228 3006 0.1667 0.2778 0.4091 0.5701 0.7693 0.49416 3121 0.3611 0.4167 0.2614 0.2422 0.8399 0.50760 3181 0.4167 0.3750 0.1402 0.3908 0.8741 0.51436 3004 0.2778 0.3333 0.4233 0.4218 0.8276 0.52342 3008 0.1667 0.3333 0.4318 0.5581 0.8033 0.52530 3184 0.3889 0.5000 0.0568 0.3785 0.8222 0.52928 3143 0.4167 0.5000 0.1506 0.2433 0.8519 0.53250 If we look at what the top 20 postcodes have in common:\nNearly all top-ranked suburbs are well-served by both tram and train lines—some with multiple options—leading to lower commute times. Suburbs like 3054 (Carlton North / Princes Hill), 3053 (Carlton), and 3002 (East Melbourne) are all within an easy, flat cycling distance to the city, explaining their high cycle scores. Inner-west and inner-north areas (3011 – Footscray, 3066 – Collingwood, 3057 – Brunswick East) show strong recent growth in income, suggesting they’re still evolving and may offer upside. While not all top suburbs have the best schools, they tend to score decently. Lower scores here may be offset by strong access and rising affluence. Suburbs like 3054, 3056 (Brunswick), and 3011 are still priced beneath the east-side powerhouses—making them viable options in terms of value-for-money. Bottom 20 Postcodes # Postcode Cycle Transit School Income Gentrification Cumulative 3154 1.0000 1.0000 0.7159 0.4502 0.5613 0.94548 3180 1.0000 1.0000 0.5417 0.6015 0.5875 0.94614 3179 1.0000 1.0000 0.4943 0.5701 0.6690 0.94668 3048 1.0000 0.8333 0.6780 0.8046 0.5939 0.94862 3153 1.0000 0.9167 0.6790 0.7280 0.5045 0.94898 3178 1.0000 1.0000 0.5369 0.6422 0.5876 0.95334 3802 1.0000 1.0000 0.5989 0.6355 0.5534 0.95756 3770 1.0000 1.0000 0.7457 0.6430 0.4080 0.95934 3059 1.0000 0.8333 0.7273 0.6502 0.7554 0.95990 3428 1.0000 1.0000 0.6170 0.5255 0.6648 0.96146 3076 1.0000 0.8810 0.6736 0.7974 0.5814 0.96288 3803 1.0000 1.0000 0.5554 0.7291 0.5692 0.97074 3062 1.0000 0.8333 0.7869 0.8981 0.5527 0.98086 3976 1.0000 1.0000 0.6051 0.6877 0.6443 0.98742 3975 1.0000 1.0000 0.7068 0.6927 0.5379 0.98748 3137 1.0000 1.0000 0.8466 0.6497 0.4587 0.99100 3063 1.0000 1.0000 0.7727 0.5351 0.6545 0.99246 3075 1.0000 0.9000 0.7121 0.8736 0.5862 0.99438 3155 1.0000 1.0000 0.7348 0.6456 0.5991 0.99590 3177 1.0000 1.0000 0.7614 0.8985 0.5239 1.03676 If we look at what the bottom 20 postcodes have in common:\nPublic transport access is consistently poor, with many areas lacking convenient tram or train connections. All bottom-ranked suburbs scored a full 1.0000 for cycling and transit, indicating long distances or poor infrastructure for both modes—likely outer suburban or fringe areas far from the CBD. School rankings vary, but tend to be mid-to-low. Even when schools are decent (e.g. 3062 – Somerton/Roxburgh Park), they\u0026rsquo;re not enough to offset poor access and distance. Affluence is moderate but stagnant, with little indication of recent gentrification. Many postcodes (e.g. 3179 – Boronia, 3154 – The Basin) have middling income and minimal growth. Gentrification signals are weak—suburbs like 3153 (Bayswater) and 3428 (Roxburgh Park) may have affordable homes, but show limited signs of transformation or upward momentum. Distance from the CBD is a common denominator. These areas might offer space, but they’re difficult to reach, especially if you don\u0026rsquo;t drive—making them less viable under a livability model weighted towards access and upside potential. Conclusion #Buying a home in a city you don’t fully understand is daunting—even more so when local knowledge is scattered, subjective, or anecdotal. My goal with this project wasn’t to find the “perfect” suburb, but to give myself a structured, data-driven way to make informed trade-offs.\nBy combining multiple datasets—transport access, school zoning, affluence, gentrification trends—I was able to narrow down the overwhelming number of options into something manageable and meaningful. The scoring model isn’t flawless, and the weights reflect my personal priorities, but it helped me move from uncertainty to clarity.\nIf nothing else, I hope this approach offers a helpful starting point for others trying to navigate the Melbourne property market with both eyes open. And if you\u0026rsquo;re a fellow spreadsheet tragic—well, now you’ve got something to plug your own assumptions into.\nHappy house hunting.\nAppendix #Download the Data #You can download all spatial datasets used in this analysis from the following links:\ncensus.gpkg — Raw census summary data cycle_zones.gpkg — Cycling time zones to Docklands gentrification.gpkg — Change in household income (2016–2021) median_age.gpkg — Median age per postcode monthly_household_income.gpkg — 2021 median household income monthly_rent.gpkg — Median rent per postcode people_per_household.gpkg — Average number of people per household poa_2021.gpkg — Postcode area geometries (2021) poa_scored.gpkg — Final scored dataset used for ranking school_zones.gpkg — Public school catchment areas train_routes.gpkg — Train route geometries train_stations.gpkg — Train station locations tram_routes.gpkg — Tram route geometries tram_stops.gpkg — Tram stop locations transit_zones.gpkg — Public transport time zones to Docklands Scoring SQL #I didn\u0026rsquo;t bother committing this query into a repo, but here it is for reference incase anyone wants to do their own weights.\nCREATE TABLE poa_scored AS WITH scored AS ( SELECT poa.poa_name21 AS postcode, (AVG(cz.value) / 6)::decimal(10, 4) AS cycle_rank, (AVG(tz.value) / 6)::decimal(10, 4) AS transit_rank, (AVG(sz.percentile_rank))::decimal(10, 4) AS school_rank, (AVG(inc.percentile_rank))::decimal(10, 4) AS income_rank, (AVG(gen.percentile_rank))::decimal(10, 4) AS gentrification_rank, poa.wkb_geometry AS geom FROM poa_2021 poa INNER JOIN cycle_zones cz ON ST_Intersects(cz.geom, poa.wkb_geometry) INNER JOIN transit_zones tz ON ST_Intersects(tz.geom, poa.wkb_geometry) INNER JOIN school_zones sz ON ST_Intersects(sz.geom, poa.wkb_geometry) INNER JOIN monthly_household_income inc ON ST_Intersects(inc.geom, poa.wkb_geometry) INNER JOIN gentrification gen ON ST_Intersects(gen.geom, poa.wkb_geometry) GROUP BY poa.poa_name21, poa.wkb_geometry ), weighted AS () SELECT scored.*, (cycle_rank * 0.2) + (transit_rank * 0.4) + (school_rank * 0.2) + (income_rank + gentrification_rank) * 0.2 AS cum_score FROM scored; Google Distances Code #This is the code I wrote to fetch distances for transit, cycle and drive. It\u0026rsquo;s more or less a one-off for me so it\u0026rsquo;s pretty messy.\nimport csv from datetime import datetime import json from pathlib import Path import googlemaps GOOGLE_MAPS_API_KEY = \u0026#34;KEY_HERE\u0026#34; POINTS = Path(\u0026#34;/Users/andryo/melb-points.csv\u0026#34;) DATA_DIR = Path(\u0026#34;./data/distances\u0026#34;) def get_travel_info( id: int, start: tuple[float, float], end: tuple[float, float] = (-37.82206849907404, 144.94578996894572), api_key: str = GOOGLE_MAPS_API_KEY, ): gmaps = googlemaps.Client(key=api_key) arrival_time = datetime(2025, 3, 21, 22, 00) travel_modes = [\u0026#34;driving\u0026#34;, \u0026#34;transit\u0026#34;, \u0026#34;bicycling\u0026#34;] for mode in travel_modes: filepath = DATA_DIR / mode / f\u0026#34;{id}.json\u0026#34; if filepath.exists(): continue directions = gmaps.directions( start, end, mode=mode, arrival_time=int(arrival_time.timestamp()), ) with open(filepath, \u0026#34;w\u0026#34;) as dst: dst.write(json.dumps(directions)) with open(POINTS, \u0026#34;r\u0026#34;) as src: reader = csv.DictReader(src, fieldnames=[\u0026#34;id\u0026#34;, \u0026#34;lng\u0026#34;, \u0026#34;lat\u0026#34;]) next(reader) points = [i for i in reader] with open(\u0026#34;distances.csv\u0026#34;, \u0026#34;w\u0026#34;) as dst: writer = csv.DictWriter( dst, fieldnames=[ \u0026#34;id\u0026#34;, \u0026#34;driving_distance\u0026#34;, \u0026#34;driving_duration\u0026#34;, \u0026#34;bicycling_distance\u0026#34;, \u0026#34;bicycling_duration\u0026#34;, \u0026#34;transit_distance\u0026#34;, \u0026#34;transit_duration\u0026#34;, ], ) writer.writeheader() for point in points: get_travel_info(point[\u0026#34;id\u0026#34;], (point[\u0026#34;lng\u0026#34;], point[\u0026#34;lat\u0026#34;])) travel_modes = [\u0026#34;driving\u0026#34;, \u0026#34;transit\u0026#34;, \u0026#34;bicycling\u0026#34;] record = {\u0026#34;id\u0026#34;: point[\u0026#34;id\u0026#34;]} for mode in travel_modes: filepath = DATA_DIR / mode / f\u0026#34;{point[\u0026#34;id\u0026#34;]}.json\u0026#34; with open(filepath, \u0026#34;r\u0026#34;) as src: data = json.loads(src.read()) if len(data) == 0: continue legs = data[0][\u0026#34;legs\u0026#34;] if len(legs) == 0: continue leg = legs[0] obj = { f\u0026#34;{mode}_distance\u0026#34;: float(leg[\u0026#34;distance\u0026#34;][\u0026#34;value\u0026#34;] / 1000), f\u0026#34;{mode}_duration\u0026#34;: float(leg[\u0026#34;duration\u0026#34;][\u0026#34;value\u0026#34;] / 60 / 60), } record = {**record, **obj} writer.writerow(record) ","date":"18 April 2025","permalink":"/posts/buyers-guide-to-melbourne/","section":"Posts","summary":"\u003cp\u003eWhen I relocated to Melbourne last September, I found myself renting again for the first time in nearly a decade. One of the hardest parts of moving to a new country was having no intuitive sense of which neighbourhoods were considered good, up-and-coming, or best avoided.\u003c/p\u003e","title":"A Data-Driven House Buyer's Guide to Melbourne"},{"content":"","date":null,"permalink":"/tags/geospatial/","section":"Tags","summary":"","title":"Geospatial"},{"content":"","date":null,"permalink":"/tags/housing/","section":"Tags","summary":"","title":"Housing"},{"content":"","date":null,"permalink":"/tags/melbourne/","section":"Tags","summary":"","title":"Melbourne"},{"content":"The catastrophic unravelling of cryptocurrency over the last year has been a sight to behold over the previous year. The perfect storm of FOMO, greed, a deafening lack of regulation and continuous and constant predatory behaviour (and people) has taken a novel concept into a \u0026rsquo;too big to fail\u0026rsquo; zero-sum game which is now starting to fail.\nZero Sum Game #Cryptocurrency had been hailed as a \u0026ldquo;revolutionary\u0026rdquo; new technology that would change the way we transact and store value. In reality, it has become a breeding ground for scams, fraud, and corruption. Despite its original promise of decentralization, transparency, and freedom from government control, cryptocurrency has become dominated by centralized exchanges, speculative trading, and other forms of manipulation.\nThe catastrophic unravelling of cryptocurrency over the last year has been a sight to behold over the previous year. The perfect storm of FOMO, greed, a deafening lack of regulation and continuous and constant predatory behaviour (and people) has taken a novel concept into a \u0026rsquo;too big to fail\u0026rsquo; zero-sum game which is now starting to fail.\nCryptocurrency is a zero-sum game, meaning that the value of a particular coin or token is determined by the collective actions of all the participants in the market, with any gain by one person being balanced by an equal loss by someone else. Cryptocurrency has no intrinsic value, and it does not have any inherent worth or usefulness.\nThe current state of cryptocurrency is a perfect example of the greater fool theory, where people are willing to pay more for an asset than its intrinsic value because they believe they can sell it to someone else at an even higher price. The conclusion of the greater fool theory is that investors who rely on this approach are likely to suffer significant losses when the market reaches a saturation point. Unfortunately for crypto, it seems that there may be no longer any \u0026ldquo;greater fools\u0026rdquo; willing to pay higher prices for the assets.\nMarket Goes Boom # \u0026ldquo;Due to market conditions, we\u0026rsquo;ve temporarily freezed withdrawals\u0026hellip;.\u0026rdquo;\n~ Celsius, Voyager, FTX, Hodlnaut, Vauld, Gemini\nCryptocurrency was supposed to be decentralized and free from government control. Still, the difficulty of trading has led to the creation of centralized exchanges (such as Celsius, FTX, etc.) that took depositor funds and lent them to others for a profit - this is banking, but this time it\u0026rsquo;s unregulated. These crypto banks\u0026rsquo; existence wholly defeated cryptocurrency\u0026rsquo;s original purpose by introducing a centralized and \u0026ldquo;trusted\u0026rdquo; party.\nThe ongoing implosion of the cryptocurrency ecosystem is the conclusion of the practical application of the greater fool theory. The sharp drop in prices in early 2022 was like a child kicking the foundation of a castle made of sand and technical jargon, causing it to come crashing down. The sharp drop in prices shattered the illusion that the market would only ever go up and led to a massive chain reaction of significant players going bankrupt. If you were invested in Bitcoin 5 years ago, you would now be down 13%.\nLine go down In June 2022, Terra (an exchange) had its \u0026lsquo;algorithmic stable coin\u0026rsquo; (paradoxical) lose its dollar peg crashing to $1.999967e-7, that\u0026rsquo;s a millionth of a dollar. A stable coin is a type of cryptocurrency that is pegged to a stable asset (e.g. a USD), and people may have invested in them because they believed they offered a safer (it didn\u0026rsquo;t) and more predictable alternative to other cryptocurrencies (it wasn\u0026rsquo;t).\nTerra Luna\u0026rsquo;s collapse, in turn, caused 3 Arrows Capital, Celsius, Voyager, BlockFi, Hodlnaut, Vauld, and FTX, to go bust in quick succession. Regrettably, this resulted in massive losses for the customers of those entities. Many of these people were likely unsophisticated investors lured in by promises of \u0026lsquo;going to the moon\u0026rsquo;.\nAt the time of writing, many other exchanges look \u0026lsquo;shaky\u0026rsquo;, with some shakier than others. It\u0026rsquo;s going to get worse.\nGood Riddance #I\u0026rsquo;m guilty of buying into the novelty of cryptocurrency in its early heyday, but now I see that it has become something different from what it was supposed to be.\nCryptocurrency in its state is an anthesis to its original concept, rife with shady and dodgy market players and scams, fraud and corruption. It\u0026rsquo;s long overdue to die. Additionally, it\u0026rsquo;s been over a decade since the \u0026ldquo;financial revolution\u0026rdquo; was meant to occur, and the only things that can be reliably bought with cryptocurrency are drugs and fake hitmen.\nThis post did not even touch on the cancer that is NFTs, but I highly recommend Dan Olson\u0026rsquo;s documentary Line Goes Up - The Problem with NFTs. There is also the fun fact that one of the largest NFT collections, Bored Ape Yacht Club, is filled to the brim with white supremacist dog whistles (whoops).\nLastly, how great would it be to stop having to hear crypto lingo finally?\n","date":"6 December 2022","permalink":"/posts/good-riddance-cryptocurrency/","section":"Posts","summary":"\u003cp\u003eThe catastrophic unravelling of cryptocurrency over the last year has been a sight to behold over the previous year. The perfect storm of FOMO, greed, a deafening lack of regulation and continuous and constant predatory behaviour (and people) has taken a novel concept into a \u0026rsquo;too big to fail\u0026rsquo; zero-sum game which is now starting to fail.\u003c/p\u003e","title":"A Fond 'Good Riddance' to Cryptocurrency"},{"content":"","date":null,"permalink":"/tags/cryptocurrency/","section":"Tags","summary":"","title":"Cryptocurrency"},{"content":"","date":null,"permalink":"/tags/public-health/","section":"Tags","summary":"","title":"Public Health"},{"content":"Few things frustrate me more than seeing anti-lockdown protestors, anti-vaxxers, or those who believe that vaccines somehow infringe upon their personal freedoms. The value of vaccination should be self-evident to anyone with even a shred of common sense.\nIn this post, I explain why getting vaccinated isn’t just important—it’s essential, and refusing to do so is both reckless and selfish.\nThe purpose of this post is for me to articulate the way I explain to others why vaccines are necessary. Unfortunately, I\u0026rsquo;ve noticed a growing trend of blatant misinformation, misunderstanding or maliciousness from people in general and a small minority of people/acquaintances that I know. Whether maliciously or not, if you know somebody in this category, I hope some of the diagrams or writing in this post helps you.\nInfection Probabilities vs. Population Vaccinated #The majority of the country understands this and are already behind getting vaccinated - however, you have a small minority which could potentially ruin things for everyone.\nThe example that I use to explain why vaccines are necessary is straightforward:\nThis graph shows the probability of getting n infected out of the ten bar patrons. The different lines show different vaccine scenarios. E.g. A wholly unvaccinated population has a 0% chance of having less than seven out of ten patrons becoming infected. In 2018, 15.2% of the New Zealand population was 65 or older. CDC has stated that you are 90 times more likely to die if you have COVID and are over 65.\nIf we treat our hypothetical bar as a sample of the New Zealand population, only in the 90% population vaccination scenario would we have a fighting chance of avoiding deaths in these older age brackets.\nTalking Points and How to Counter Them #In this section, I\u0026rsquo;ve listed down the main arguments I often hear being repeated. These arguments sometimes occur sequentially after being debunked, or they may be obfuscated with other red herrings. However, in general, I find these as the most common.\nWhy Should I If Everyone Else Is? #Getting the population vaccinated helps stop the spread of COVID-19, but vaccinations do not give total immunity to the virus. It only reduces the chance of transmission (and severity of symptoms if contracted). Every person who does not get vaccinated adds to the pool of infectious people in the community, potentially infecting others (even those vaccinated).\nYou can also point out having this stance is incredibly selfish.\nForcing Me to Vaccinate Infringes on My Freedom #This idea of a \u0026ldquo;loss of freedom\u0026rdquo; is most likely the unfortunate consequence/symptom of the American media machine slowly influencing New Zealand in the worst possible way. In a civilised society, the personal freedoms we enjoy come with strings; these strings are doing the right thing for the collective good.\nI found this quote online, and I think it articulates the subject far better than I could.\n\u0026ldquo;The government shouldn\u0026rsquo;t be telling us what to do,\u0026rdquo; they say, as their WOF-certified car rolls to a stop at a red light on their way to their MBIE-protected jobs.\n\u0026ldquo;They\u0026rsquo;re not in charge of my health,\u0026rdquo; they continue as they chomp into their NZFS-rated bagel with the confidence of one who knows it won\u0026rsquo;t send them to the taxpayer-funded hospital, but even if it did, the hospital has to treat them regardless of funds thanks to MOH policy.\n\u0026ldquo;We deserve the freedom to make our own choices,\u0026rdquo; they conclude, as the green light flicks on to inform them they can go. \u0026ldquo;\u0026amp;$*# you, eco-!@$#!\u0026rdquo; They curse at a nearby bicyclist for swaying a bit too close to their bonnet.\nVaccines Are Dangerous #Many people have valid health issues and concerns which prohibit them from getting the vaccination. The people who cannot get vaccinated will be protected most by a highly vaccinated population. That\u0026rsquo;s why it makes it so much worse that some people erroneously (intentional or not) use this as their excuse for why they will not get vaccinated.\nCDC\u0026rsquo;s reaction and adverse events register has catalogued the number of severe adverse reactions caused by the Pfizer vaccine. Out of the 7,851 doses given (all age groups \u0026gt; 12), there were 0 \u0026ldquo;Grade 4\u0026rdquo; events, these are the events that would require you to be hospitalised.\nThe proportions of participants who reported at least 1 serious adverse event were 0.4% in the vaccine group and 0.2% in the placebo group. No serious adverse events were considered by FDA as possibly related to vaccine.\nYou are more likely to get into a car crash than getting a severe reaction to the vaccine.\nConclusion #Get vaccinated.\n","date":"30 October 2021","permalink":"/posts/vaccinations-matter/","section":"Posts","summary":"\u003cp\u003eFew things frustrate me more than seeing anti-lockdown protestors, anti-vaxxers, or those who believe that vaccines somehow infringe upon their personal freedoms. The value of vaccination should be self-evident to anyone with even a shred of common sense.\u003c/p\u003e","title":"Vaccinations Matter (aka Why Anti-Vaxxers Should Really Reconsider)"},{"content":"","date":null,"permalink":"/tags/vaccines/","section":"Tags","summary":"","title":"Vaccines"},{"content":"","date":null,"permalink":"/tags/hugo/","section":"Tags","summary":"","title":"Hugo"},{"content":"The biggest strength of a static pages blog is that it\u0026rsquo;s all pre-rendered; your website will load lightning-quick and give you great SEO; however, it becomes a massive chore when you attempt to integrate external projects.\nA couple of months ago, I thought it would be a fun project to create my own CMS/blog to replace the static pages (Hugo) blog I currently had deployed on my GitHub Pages. My primary motivator for doing this was my perceived lack of flexibility of Hugo, which was the lack of flexibility outside of written content.\nI let it run for a couple of months before I decided the other day that I\u0026rsquo;d had enough and reverted to my Hugo blog.\nThe Challenge #I went with a Django (with DRF) backend, React (with RTK) frontend, and PostgreSQL for this particular project - this combination is probably my project favourite stack. With a little bit of tweaking, you can get a project up and running in no time at all. While I love this project stack, this experience has taught me a valuable lesson. There is no such thing as a one-size-fits-all solution.\nArchitecture #An application made with React can be incredibly dynamic and flexible. Still, one of its biggest problems is that it is an incredible pain in the ass to do the SEO and performance comparable to that of a static page.\nServer-side rendering (SSR) becomes a necessity when you dynamically set meta tags for your content; this means that the minimum architecture you would need to deploy a React CMS/blog is as follows:\nThe application flow works out to be something like this:\nA user requests to visit a page NGINX routes the request to the relevant parts of the application The React SSR needs to render the React page and relevant meta tags before returning it to the user. The SSR requests the metadata from the backend. The metadata is retrieved from the backend and injected into the HTML response. Both the backend and SSR return their respective responses to NGINX. NGINX routes through the appropriate reverse proxy, and the user receives the rendered page. If it looks overly complicated, that\u0026rsquo;s because it is. In hindsight, I may have over-engineered this, but I couldn\u0026rsquo;t develop another way to make this reliably work. If someone smarter than me has an easier way to achieve what I was trying to do, please reach out to me and let me know.\nWhen I checked the website on Google to see how it performed, it performed awfully. Too many things were happening behind the scenes, which resulted in sluggish performance. It\u0026rsquo;s just not comparable to a static page where all the content is ready to go.\nOther Issues #Beyond performance, there are other costs to doing this approach, making the whole thing even more unappealing. Depending on your stance and preferences, you may consider me unreasonable here, but hear me out.\nDeployment Overhead #Naturally, having a Django/React application was more resource-intensive in regards to deployment. I self-host my applications, so the cost, in this case, was not monetary. Instead, it was the administrative overhead of deploying this project. In this case, we have even more overhead than a standard Django/React application as we need to worry about the SSR element of the project.\nIt\u0026rsquo;s tough to compete with the almost no-overhead static pages type blog, especially when paired with something like GitHub pages which makes the process almost braindead.\nIn my case, when I want to make changes to the blog, I execute a simple shell script:\n#!/bin/bash shopt -s extglob cd marzukia.github.io/ git checkout master rm -rv !(\u0026#34;CNAME\u0026#34;|\u0026#34;.git/\u0026#34;) cd .. hugo cd marzukia.github.io/ git add . git commit -m \u0026#39;deploy changes\u0026#39; git push cd .. cd themes/salt git add . git commit -m \u0026#39;deploy changes\u0026#39; git push cd ../.. git add . git commit -m \u0026#39;new post or changes\u0026#39; git push As soon as I execute the script, it\u0026rsquo;ll run Hugo to build the static pages, recursively go into each submodule I have associated with the project, push changes, and we\u0026rsquo;re good to go. While in general, I would not consider the deployment overhead a factor, given the alternative is so easy, it becomes a pretty material consideration.\nOngoing Development \u0026amp; Maintenance #Why start a project if you aren\u0026rsquo;t going to maintain it? Having an application that has a backend, frontend and database will, of course, require more love and attention than a static page which is just HTML and CSS. This complaint seems like a stupid thing to whinge about, but given the purpose of my blog, I couldn\u0026rsquo;t justify it.\nMy blog serves two purposes for me:\nThe blog acts as a diary of sorts to help me crystalise my thoughts and retrospectively review things I\u0026rsquo;ve done. The blog acts as a tool to help me build and maintain a social media presence in the industry. Ultimately, it came back to \u0026ldquo;this is not worth it\u0026rdquo; for me.\nWhy Do It? #I\u0026rsquo;ve been fairly negative in this article, but I did allude to having reasons for doing this in the first place.\nDynamic Pages \u0026amp; Data #I absolutely love creating exciting data visualisations and tools. One significant benefit of building the project in Django/React meant that I had a much more sophisticated toolset to work with in terms of the content I could produce. The tool that I had packaged with the release of my React blog was a game-price finder that analysed scraped data from Wata Games/Heritage Auctions.\nNote: This tool will be down at the time of writing, but I\u0026rsquo;ll look at rebuilding it without the blog component. Also, you can find the aforementioned article here.\nI will make standalone applications in the future rather than trying to have it all unified as a single experience.\nGood Practice \u0026amp; Self Reflection #I recently discovered RTK Query this year, and I love it. I haven\u0026rsquo;t had much time to explore the full capabilities, so I thought this project would be great practice.\nAnother huge thing for me was the opportunity to self-reflect on the progress I\u0026rsquo;ve made as a developer. Every time I do a Python, TypeScript, or C# project, I use it as a way to see how my overall code quality has improved. Ultimately, this blog rework was a bit of a pointless exercise. The project itself may not be valuable, but the lessons learned are valuable and immediately apply to my personal and professional work.\nIt\u0026rsquo;s also never a bad thing to have too much code in your GitHub. There\u0026rsquo;s no doubt in the future, and I may run into a problem that will require a solution that I\u0026rsquo;ve already come up with.\nClosing Thoughts #I don\u0026rsquo;t know where I was going with this blog post - but if you\u0026rsquo;ve managed to make it this far, I hope it was at least somewhat interesting. If you were considering making a React blog and I\u0026rsquo;ve successfully stopped you, I will take that as a win.\nIn conclusion, don\u0026rsquo;t bother trying to make React blog, it sucks, and it\u0026rsquo;s painful to do. I\u0026rsquo;m open to changing my mind here, but for now, this will be my position.\nI\u0026rsquo;ll close this section off with a self explanatory picture as to why static page blogs kick ass.\nServer-Side Rendering Code #If you stumble across this page sometime in the future and you want to know how I set up my SSR, I\u0026rsquo;ve placed the code snippet below.\nI\u0026rsquo;m going to add a disclaimer here that I do not have much familiarity with NodeJS, nor do I work in plain JavaScript very often. There will be a certain level of \u0026ldquo;jank\u0026rdquo; that I won\u0026rsquo;t be able to hide in the following code snippet.\nimport express from \u0026#34;express\u0026#34;; import path from \u0026#34;path\u0026#34;; import fs from \u0026#34;fs\u0026#34;; import fetch from \u0026#34;node-fetch\u0026#34;; var app = express(); const __dirname = path.resolve(); const port = process.env.PORT || 1235; app.use(express.static(path.resolve(__dirname, \u0026#34;./build\u0026#34;))); app.get(\u0026#34;/\u0026#34;, function (request, response) { const filePath = path.resolve(__dirname, \u0026#34;./build\u0026#34;, \u0026#34;index.html\u0026#34;); fs.readFile(filePath, \u0026#34;utf8\u0026#34;, function (err, data) { if (err) { return console.log(err); } // replace the special strings with server generated strings data = data.replace(/\\$OG_TITLE/g, \u0026#34;Andryo Marzuki - Data \u0026amp; Analytics\u0026#34;); data = data.replace( /\\$OG_DESCRIPTION/g, \u0026#34;Data analytics \u0026amp; personal blog of Andryo Marzuki\u0026#34; ); response.send(data); }); }); app.get(\u0026#34;/about\u0026#34;, function (request, response) { const filePath = path.resolve(__dirname, \u0026#34;./build\u0026#34;, \u0026#34;index.html\u0026#34;); fs.readFile(filePath, \u0026#34;utf8\u0026#34;, function (err, data) { if (err) { return console.log(err); } data = data.replace(/\\$OG_TITLE/g, \u0026#34;Andryo Marzuki - About\u0026#34;); data = data.replace( /\\$OG_DESCRIPTION/g, \u0026#34;About Andryo Marzuki, career history and interests.\u0026#34; ); response.send(data); }); }); app.get(\u0026#34;/contact\u0026#34;, function (request, response) { const filePath = path.resolve(__dirname, \u0026#34;./build\u0026#34;, \u0026#34;index.html\u0026#34;); fs.readFile(filePath, \u0026#34;utf8\u0026#34;, function (err, data) { if (err) { return console.log(err); } data = data.replace(/\\$OG_TITLE/g, \u0026#34;Andryo Marzuki Contact Details\u0026#34;); data = data.replace( /\\$OG_DESCRIPTION/g, \u0026#34;Contact details for Andryo Marzuki\u0026#34; ); response.send(data); }); }); app.get(\u0026#34;/posts/*\u0026#34;, function (request, response) { const filePath = path.resolve(__dirname, \u0026#34;./build\u0026#34;, \u0026#34;index.html\u0026#34;); fs.readFile(filePath, \u0026#34;utf8\u0026#34;, function (err, data) { if (err) { return console.log(err); } const doAsync = async () =\u0026gt; { const meta = await fetch( `https://api.mrzk.io${request.url}?meta=True` ).then((response) =\u0026gt; response.json()); data = data.replace(/\\$OG_TITLE/g, \u0026#34;Andryo Marzuki - \u0026#34; + meta.title); data = data.replace(/\\$OG_DESCRIPTION/g, meta.description); data = data.replace(/\\$OG_URL/g, `https://mrzk.io${request.url}`); data = data.replace(/\\$OG_PUBLISH_TIME/g, meta.createdDate); response.send(data); }; doAsync(); }); }); app.get(\u0026#34;*\u0026#34;, function (request, response) { const filePath = path.resolve(__dirname, \u0026#34;./build\u0026#34;, \u0026#34;index.html\u0026#34;); fs.readFile(filePath, \u0026#34;utf8\u0026#34;, function (err, data) { if (err) { return console.log(err); } data = data.replace(/\\$OG_TITLE/g, \u0026#34;Andryo Marzuki - Data \u0026amp; Analytics\u0026#34;); data = data.replace( /\\$OG_DESCRIPTION/g, \u0026#34;Data analytics \u0026amp; personal blog of Andryo Marzuki\u0026#34; ); response.send(data); }); }); app.listen(port, () =\u0026gt; console.log(`Listening on port ${port}`)); ","date":"29 October 2021","permalink":"/posts/react-blog-and-regret/","section":"Posts","summary":"\u003cp\u003eThe biggest strength of a static pages blog is that it\u0026rsquo;s all pre-rendered; your website will load lightning-quick and give you great SEO; however, it becomes a massive chore when you attempt to integrate external projects.\u003c/p\u003e","title":"Making a React Blog and Why You Shouldn't (They Suck)"},{"content":"","date":null,"permalink":"/tags/react/","section":"Tags","summary":"","title":"React"},{"content":"","date":null,"permalink":"/tags/seo/","section":"Tags","summary":"","title":"SEO"},{"content":"","date":null,"permalink":"/tags/ssr/","section":"Tags","summary":"","title":"SSR"},{"content":"","date":null,"permalink":"/tags/static-site/","section":"Tags","summary":"","title":"Static Site"},{"content":"","date":null,"permalink":"/tags/typescript/","section":"Tags","summary":"","title":"TypeScript"},{"content":"","date":null,"permalink":"/tags/diy/","section":"Tags","summary":"","title":"DIY"},{"content":"","date":null,"permalink":"/tags/mechanical-keyboard/","section":"Tags","summary":"","title":"Mechanical Keyboard"},{"content":"When Auckland went into lock-down in August 2021, I decided to get myself a custom keyboard after being suckered in by various videos by Glarses and Hipyo Tech. I spend so much time at my computer for work and personal projects I thought it\u0026rsquo;d be a worthwhile endeavour to make something I could enjoy 12+ hours a day.\nThis post will primarily serve two purposes. I\u0026rsquo;m writing about it just for my record, and the second is to help any other would-be entrant to the hobby to outline my key learnings and share things I would\u0026rsquo;ve wanted to know before I started.\nMy Keyboard #My keyboard itself is what I\u0026rsquo;d call \u0026ldquo;semi-finished\u0026rdquo;. I purchased a lot of stuff currently sitting at the office. As I\u0026rsquo;m not allowed to return to work until the pandemic dies down a bit, I had to reorder some parts to use the keyboard in the short term. The postal service (NZ Post) also managed to make my life a living hell during this lockdown by actually losing my GMMK pro and delaying every package by weeks at a time.\nAt the time of writing, this is my setup:\nGMMK Pro Hotswappable Keyboard C3 TKC Tangerines Glorious Rainforest PBT Keycaps Durock V2 Stabilisers Durock Switch Films I’m waiting on some MT3 keycaps and some other neat stuff to bolt on this keyboard, once I eventually receive those, I’ll likely make another post.\nWhy Make a Custom Mechanical Keyboard? #Suppose you ever accidentally get suggested a custom mechanical keyboard video on YouTube. In that case, your immediate reaction will probably be something along the lines of \u0026ldquo;What the hell is this person talking about\u0026rdquo;. The hobby looks at assembling and modifying custom keyboard components to achieve a fantastic aesthetic look and sound.\nA custom mechanical keyboard is like any other luxury item; they are ultimately pointless. Just like buying a fancy watch, you get it because the aesthetics and feel of the product feel better despite having the same function as a membrane keyboard. Building a custom mechanical keyboard lets you tailor your keyboard to exactly your taste and preferences, which I think is pretty cool.\nThe Summary #I ran into a few massive hurdles straight away that made entry into the hobby a little more challenging than I expected.\nThe first one was the steep learning curve of the hobby. Being located in New Zealand meant that everything was insanely expensive or a pain in the ass to get. Lastly, anytime I watched a video, I instantly got buyers regret, cancelled my order and ordered an upgraded version of whatever I was purchasing. One of the barriers to this hobby is the culture, vocabulary, and knowledge built around the subject. As an outsider looking in, it was pretty challenging to get a base understanding of what was going on and discussed.\nMy initial reaction to the custom mechanical keyboard community is that it\u0026rsquo;s apparent that everyone who partakes in the community has a zeal about mechanical keyboards, which is very admirable. However, there is pretty abundant and conspicuous elitism in the community, which may be off-putting for some.However, the good news is that as you dive a bit deeper into the community, you find that the elitism is over represented. There have been multiple times where I\u0026rsquo;ve asked for help or advice, and I received beneficial and friendly counsel each time.\nOnce you get your head around the terminology, the mechanics of assembling and modifying a keyboard is straightforward. If you purchase a hot-swappable keyboard like me, the only thing you\u0026rsquo;ll need to learn is how to lubricate switches and stabilisers.\nThe Lingo #To cut things short, as a newbie to the subject, here are some of the key phrases that you need to know\nSwitch - These are what the keycap sits on top. Plate - The plate sits on top of the PCB. PCB - This is the actual circuit board of the keyboard. Lube - Lubricant. Don\u0026rsquo;t watch custom keyboard videos in front of your spouse, or they\u0026rsquo;ll ask you, \u0026ldquo;Why does that person keep talking about lube\u0026rdquo;. Krytox 205G0 - This is the gold standard of switch and stabiliser lubricant. It\u0026rsquo;s a viscous lubricant that you\u0026rsquo;ll be applying liberally on everything. Krytox GPL 105 - This is what you imagine when you think of lubricant. Very liquid-y, I used this to lubricate my springs. Stab / Stabiliser - Stabilisers are attachments that surround your modifiers keys and your space bar. Because these keys are big, these stabilisers provide support for those keys. KBD - KBDFans - they sell lots of keyboard stuff. PBT - PBT is a type of plastic used for keycaps. These keycaps have more of a premium feel than ABS plastic. They don\u0026rsquo;t shine over time and sound different than ABS keycaps. ABS - These are the keycaps you\u0026rsquo;ll be used to. These come standard with 99% of keyboards you can buy at a store. They shine over time and have a \u0026lsquo;wetter\u0026rsquo; feel. QMK/VIA - A type of open-source keyboard firmware that allows you to remap your keyboard easily. Hot-swappable - A type of PCB that doesn\u0026rsquo;t require soldering. You can plug your switches directly into the PCB. Films - Switch films are a plastic sheet that sits between the top and bottom housing of your switch to make the switch sound and feel better. Stem Holder - The stem of a switch is super tiny, and if you have big hands like me, they are awful to hold. A stem holder lets you clamp onto the switch and hold it. I used a mechanical pencil for my stem holder. Linear - Linear switches give you a smooth downwards press. Tactile - Tactile switches have a little bit of a bump in the middle of the end of a keypress. The typical examples you can find on store-bought keyboards would be MX Brown and Blue. Group Buys - When you’re looking for key cap sets for your keyboard you may across the term ‘group buys’. These are massive expressions of interest/preorders for things that haven’t been manufactured yet. Expect to wait for a long time if you’re doing one of these. Lubricating \u0026amp; Filming Switches # Lubricating and filming a switch improves the smoothness and the sound of the switch. The entire process took me approximately four hours; it was an incredibly tedious process, and honestly, I hated it.\nI won\u0026rsquo;t detail how to lubricate a switch as there are already some great online resources that go into these in-depth. Here is my viewing recommendation.\nThe Takeaway #Rather than explain how to lube a switch, I thought I\u0026rsquo;d share my key learnings from lubing all my switches:\nOrder spares. I broke about eight switches during the trial-and-error phase of assembling the keyboard. If you need 84, buy at least 90 to give yourself some breathing room. Do the process in batches. There are various steps in lubricating a single switch; completing each step for 10-15 switches at a time helped significantly with my consistency. Doing the entire process for a single switch one at a time made me less consistent. Over-lubing is a lot less of a risk if you have linears (as opposed to tactiles). The conventional wisdom is that you should use lube sparingly the first time you lube switches; I went ham with my lubing, and it turned out alright. If you have the disposable income, I’d seriously recommend you just to buy lubed/filmed switches from Etsy or something. With that being said, it was a cool experience overall (despite how tedious it was). Tuning the Stabilisers # Replacing the stabilisers on my GMMK Pro was honestly a bit of a nightmare. If you\u0026rsquo;ve done some pre-order reading, you\u0026rsquo;ll see many people recommend replacing the subpar factory GOAT stabilisers with Durock v2 stabilisers or something similar. You may also read that the plate doesn\u0026rsquo;t fit well when you have these stabilisers in.\n\u0026ldquo;Doesn\u0026rsquo;t fit well\u0026rdquo; is a vast understatement of how much of a pain in the ass it is to reassemble the GMMK pro with non-factory stabilisers. Frankly, the plate will not fit at all without a gentle amount of \u0026ldquo;sweaty forcing the plate onto the PCB\u0026rdquo;.\nThe Takeaway #So my advice to those who\u0026rsquo;ve purchased a GMMK Pro and decided to replace the factory stabilisers:\nIgnore that part of your brain that\u0026rsquo;s screaming, \u0026ldquo;you\u0026rsquo;re breaking it!\u0026rdquo; and keep forcing that plate onto the board. Either align the top or bottom tabs, then work your way downwards or upwards. Starting in the middle will not work. Don\u0026rsquo;t panic when you see the number of tiny screws in the GMMK pro. In regards to lubing and tuning the stabilisers, I\u0026rsquo;d recommend this great video.\nFlashing QMK onto GMMK Pro on Linux #There isn’t many guides on how to use QMK with Linux so I thought I’d do a quick write up on how to do it.\npip3 install qmk qmk setup Set up will create a folder at ~/qmk_firmware. This will download all required dependencies and other things that qmk needs.\nsudo cp /home/andryo/qmk_firmware/util/udev/50-qmk.rules /etc/udev/rules.d/ ## reboot after this qmk config user.keyboard=gmmk/pro/ansi qmk new-keymap vim ~/qmk_firmware/keyboards/gmmk/pro/ansi/keymaps/\u0026lt;name\u0026gt;/keymap.c At this point, you’ll see something like this:\nOnce you’ve modified your keybinds to your preference, load the keyboard into bootloader mode and then run the following:\nqmk flash Closing Thoughts #That’s all the salient thoughts I had jotted down in regards to my first custom keyboard build. Overall, I really enjoyed it and can’t wait to do it again. My wife wants a custom keyboard now so I have the excuse to build another fairly soon.\nI hope for those who stumble across this post, I hope the information provided has been useful.\nGood luck!\n","date":"28 October 2021","permalink":"/posts/first-custom-keyboard/","section":"Posts","summary":"\u003cp\u003eWhen Auckland went into lock-down in August 2021, I decided to get myself a custom keyboard after being suckered in by various videos by \u003ca href=\"https://www.youtube.com/channel/UCFtOX-21N1earf-K58C7HjQ\" target=\"_blank\" rel=\"noreferrer\"\u003eGlarses\u003c/a\u003e and \u003ca href=\"https://www.youtube.com/channel/UCXlDgfWY2JbsYEam2m68Hyw\" target=\"_blank\" rel=\"noreferrer\"\u003eHipyo Tech\u003c/a\u003e. I spend so much time at my computer for work and personal projects I thought it\u0026rsquo;d be a worthwhile endeavour to make something I could enjoy 12+ hours a day.\u003c/p\u003e","title":"My First Custom Keyboard - Learnings \u0026 Advice to Newbies"},{"content":"It\u0026rsquo;s not often that three of my interests mash intersect together, in this case, my love of video games, data and technology. So you can imagine my excitement when I discovered a scandal in the making currently brewing in the collectable gaming world. Two significant players are colluding to inflate prices, causing a bubble rapidly.\nThis post will go in-depth about the unravelling situation involving Heritage Auctions and Wata (a game grading company) and talk about the evidence in the data supporting the theory that collusion/market manipulation is occurring.\nI should be clear; this post has been written purely for educational purposes. While I\u0026rsquo;ve used data collected from Heritage Auctions, no copyright infringement is intended.\nThis article will be pretty long. If you can\u0026rsquo;t read through everything, check the \u0026quot;Key Findings\u0026quot; section for a summary of the findings.\nAs part of this mini-project, I\u0026rsquo;ve made a tool that provides a quasi-population report for any collectable game sold at Heritage Auctions, you can find this tool here. The data I\u0026rsquo;ve collected from Heritage Auctions is also freely available for anyone to use for their purposes.\nThis data is now available for anyone to use and collected as of the 5th September 2021.\nBackground #The video game collectables market has recently seen an insane increase in prices, with a copy of Super Mario 64 selling for an absurd US$1.6M. To put things into perspective, this exact copy of Super Mario 64 sold for approximately US$30K a few years ago. To reiterate, a video game that has sold 11 million times just sold for US$1.6M\u0026hellip;\nA key driving force of this insane price spike is thanks to two companies\u0026rsquo; efforts, Heritage Auctions and Wata Games. Their behaviour and actions strongly suggest that they are manipulating the market.\nWata Games \u0026amp; Heritage Auctions #Wata Games is a video gaming grading agency, a company that values an item based on its overall condition and whether it\u0026rsquo;s a loose cartridge, completed in the box, sealed, etc. In their own words, Wata Games describes their services as:\nWe have fair, objective grading standards you can count on to help assess a game\u0026rsquo;s condition and authenticity with the utmost confidence in your purchase.\nGiven Wata Game\u0026rsquo;s role in the collectable video game market, it\u0026rsquo;s clear that they significantly influence the price of collectable games. It quickly becomes alarming when you discover that a co-founder of Heritage Auctions (Jim Halperin) sits on the advisory board of Wata Games.\nJim Halperin, coincidentally, has previously been sued by the Fair Trade Commission (and was essentially fined $1.2M) for purposefully inflating the prices of collectable coins.\nKey Findings #To date, Wata Games has not released a population report. A population report is a critical piece of information that quantifies games that have been graded/assessed to a specific grade. Not releasing this information means that the overall rarity of any given video game is incredibly opaque and can lead to distortions of perceived value. To make things worse, Heritage Auctions does not release the details of buyers or sellers of listings leading to even more room for shady things to happen.\nIn my analysis of the data I collected, I think I can summarise my findings to the following points:\nRecent sales of video games at ridiculous prices have distorted the perception of video game prices. The average price of video games has been trending downwards over the last two years. These \u0026lsquo;headline\u0026rsquo; sales are extreme outliers when viewed concerning the distribution of other video game sale prices. Only a handful of games have sale prices more significant than the six-figure mark, and there is seemingly no relationship with a given video game\u0026rsquo;s overall popularity. While not definitive, the data strongly suggests that these \u0026lsquo;headline\u0026rsquo; sales are being used as a mechanism to inflate the prices of games artificially, or at the very least, the perception of said games. In conclusion, something fishy is going on here.\nAs I\u0026rsquo;ve stated at the start of the article, the data I\u0026rsquo;ve collected is available for download here.\nGame Inflation \u0026amp; Activity #If the market were genuinely surging, I\u0026rsquo;d expect that the max sale price of a collectable game would have a uniform deviation from their average price. If we look at the top ten video games with the highest deviation from their average price we find the following: If you\u0026rsquo;re at all familiar with video games, these names should be very familiar as they are either culturally iconic games, sold millions of copies, or both.\nHowever, when we look at the maximum prices of these games, we see some very striking outliers. In particular, we have three games (Super Mario 64, Super Mario Bros, The Legend of Zelda) with extremely high sale prices compared to other games in the top ten.\nIf we plot the relationship of these ten games regarding their lifetime copies sold versus their maximum sale price, there is no clear pattern or relationship visible. Granted, the population for this visualisation is small.\nIf we look at all games sales from the available data, we can see some clear outliers. In particular, lets zone in on two of these four sales:\nSuper Mario Bros Super Mario 64 The Legend of Zelda The justification for the extreme price premium on these games is that they have some rarity element, i.e. early copies or a missing trademark.\nHowever, is this rarity precious enough to increase the prices tenfold to twentyfold the norm?\nWhether or not this scarcity truly justifies the crazy price tag, what\u0026rsquo;s certain is that comments such as \u0026quot;The last year has seen the market for retro games increase tenfold.\u0026quot; would achieve what someone attempting to manipulate the market would want to do.\nThese comments would generate the perception/news coverage that the video game collectable market is the next big thing.\nMarket Volumes \u0026amp; Trends #If we were to plot the total value of market activity on a time series and looked over the last two years, we\u0026rsquo;d see a market with an almost exponential growth curve in terms of value. This is consistent with the narrative you\u0026rsquo;d be expected to buy in, the video game market is scorching right now, and you should buy games.\nWhen we look at the distribution of sale prices, we can quickly see that the headline sales, which have caused so much enthusiasm, make an extremely small amount of total sales. A tiny fraction of total sales is between $30K and $1.6M, with \\~99% under $10K.\nIf we instead look at average prices of sales, we see the actual story. On average, video game prices have been trending downwards. If your goal is to inflate games to make a tidy profit, this is not something you want to advertise.\nFurther Reading #If you\u0026rsquo;d like more information about the key players and accusations, the following are great pieces of media:\nThis Kotaku article provides an excellent written summary. Karl Jobst\u0026rsquo;s video covering the situation covers the topic very well. If you have an hour to kill, I would recommend a watch. ","date":"11 September 2021","permalink":"/posts/collectable-video-games-market-manipulation/","section":"Posts","summary":"\u003cp\u003eIt\u0026rsquo;s not often that three of my interests mash intersect together, in this case, my love of video games, data and technology. So you can imagine my excitement when I discovered a scandal in the making currently brewing in the collectable gaming world. Two significant players are colluding to inflate prices, causing a bubble rapidly.\u003c/p\u003e","title":"Collectable Video Games \u0026 Market Manipulation"},{"content":"I\u0026rsquo;ll be the first to admit that I\u0026rsquo;m not the greatest fan of politics, in a typical scenario I\u0026rsquo;d avoid talking about it like the plague. However, as the elections have come and gone, it\u0026rsquo;d be sacrilege for me to pretend the goldmine of electoral data now available did not exist. In this case, my love for all things data outweighs my disdain for politics. So I thought, why not make it fun?\nIn Basketball, there is a litany of metrics which can give a different perspective on how good a player is. There are standard metrics like Points per Game, or Assists per Game which give a pretty obvious indicator of a player\u0026rsquo;s contribution to a game, and more detailed measurements such as \u0026lsquo;Offensive Rating\u0026rsquo; which are far more complex. For example, \u0026lsquo;Offensive Rating\u0026rsquo; attempts to paint an overall picture of how well a player plays offensively by not only his points contribution but things like how well he passes or how well he sets screens for a play.\nSo in the theme of treating the elections like a sport, our \u0026lsquo;plain jane\u0026rsquo; metrics would be votes received and seats gained. What about our complex or advanced metrics? What can we do to get a better idea of the \u0026lsquo;full picture\u0026rsquo;?\nNote: Please keep in mind that there are quite a few assumptions which have been made, as well as limitations of the data itself. For my methodology, approach and commentary on decisions around the underlying data, please see the end of this article.\nDonation Utilisation Efficiency #Measures which explicitly look at the return on hard-earned dollars are the norm in almost every industry; it\u0026rsquo;s natural to want to understand what you\u0026rsquo;re getting out of spending your hard-earned dollar(s). For a political party, the equivalent of money going in would be donations received from the government and their private supporters.\nNote: Political parties have to disclose what they receive in donations each year. Unfortunately, at the time of writing this data was not available for 2020. The figures discussed in this section are the sum of contributions received by each party in 2018 and 2019.\nAs a banker, I love my acronyms. Therefore I have devised a new one\u0026rsquo; Donation Utilisation Efficiency\u0026rsquo;, or DUE for short. DUE looks at the ratio of donations received per party, and the overall results achieved by the said party; this metric looks at both seats and votes acquired.\nThis metric intends to do the following:\nAct as my proxy to quantify the overall \u0026lsquo;brand power\u0026rsquo; of each party (votes) Describe how effectively they\u0026rsquo;ve translated party donations into actual power (seats) The graph is a summary of the main parties, and how well they\u0026rsquo;ve translated their donation funds into both votes and seats.\nVoter Age Demographics #Age plays a significant factor on who you would likely vote for; this is simply because your generation will dictate things like your phase in life, asset ownership or general financial security.\nThere wasn\u0026rsquo;t much that I wasn\u0026rsquo;t expecting when seeing the results, key take-aways being:\nThe left-leaning parties (Green and Labour) have far greater representation for the younger generations (Gen X and Millenials). Conversely, the right-leaning parties, have far greater representation of the older generation. Voter Housing Demographics #The housing crisis in New Zealand is one of the most significant issues that New Zealand currently faces, so naturally, I wanted to make a graph on that too.\nThere are some interesting that instantly pop out to me when I view this data:\nLabor has a massive proportion of their voter base that do not own homes; I was incredibly surprised by this given that Labor\u0026rsquo;s position on the matter is \u0026lsquo;keep house prices as they are\u0026rsquo;. The majority of National and ACT supports own their own homes, so it feels intuitive that the housing crisis would not be as front of mind as it would be for Green/Labour supporters. Power Rankings #In the next few sections, I\u0026rsquo;ll attempt a \u0026lsquo;power ranking\u0026rsquo; of sorts of the four political parties noted.\n1. Labour #It\u0026rsquo;s not an easy feat to have a single majority in an MMP government, but Labour has done it. Regardless of how you view Labour\u0026rsquo;s policy, there\u0026rsquo;s no denying that Jacinda Ardern has been an excellent crisis leader over the last three years handling the significant incidents like the Christchurch Mosque Shootings, White Island Eruption, and now the COVID-19 pandemic very well.\nI\u0026rsquo;d say that Jacinda is more or less Labour at this point, and the DUE reflects that. The cost for Labour to influence voters and convert their relatively lesser resources into seats is significantly less than their competition.\nLabour has some significant challenges in their near future with some of the key ones being:\nHandling of the COVID19 recovery both economically and socially. Would be continued and prolonged lockdowns erode the public goodwill in the Labour\u0026rsquo; brand\u0026rsquo;? Opposition from both the right and left will likely cause additional pressure Labour did not experience in their last term. Actually dealing with the housing crisis. 80% of their voters do not own their own home; it\u0026rsquo;s a severe no brainer to deal with this if they want to retain their voting base. Labour\u0026rsquo;s position is more or less \u0026lsquo;keep the status quo\u0026rsquo; - maybe that\u0026rsquo;s just what New Zealanders want?\n2. National #It couldn\u0026rsquo;t have been easy to be the opposition to a party which had recently successfully contained a pandemic. The constant string of internal bickering, leaks and controversies also did not help.\nTheir perfect storm of missteps and misfortune has predictable results:\nRight-leaning national supports fled in droves to the likes of ACT and New Conservative. In an attempt to recover these voters, National shifted further right in their policies around tax breaks, gun control, and border control causing their more centrist supporters to shift inwards towards Labour. This resulted in National seeing a massive decline losing 21 seats between elections, long-held seats such as Ilam were lost to Labour. Despite only receiving 26% of votes, National by far has the most significant amount of donations with nearly double the next highest party. Compared to their greatest rival, National\u0026rsquo;s DUE was three times greater than Labour\u0026rsquo;s indicating it was far more challenging for them to influence voters to vote for them or rather voters were already more inclined to vote for Labour over National.ye In summary, National did bad - but I didn\u0026rsquo;t have to tell you that did I?\n3. Greens #I\u0026rsquo;ve placed Greens ahead of ACT as I feel that Labour would be much more willing to deal with the Greens due to their closer ideologies.\nGreens see themselves with more seats this time around. However, they will likely no longer be part of the acting government for this electorate. Instead, I predict their role this time around will be as the opposition voice for the left.\nThey\u0026rsquo;ve had their share of missteps (ahem, James Shaw). Still, they\u0026rsquo;ve also done some notable things such as negotiating the Cannabis Referendum as part of their confidence and supply agreement, and introduction of the Zero Carbon Bill.\nAnecdotally, I\u0026rsquo;ve heard that Green supporters tend to be more socially conscious younger people with decent incomes. The donations received by Greens reflect this, receiving nearly as much as Labour with a tenth of their votes. However, this doesn\u0026rsquo;t necessarily translate to an increase in votes or seats.\nTheir DUE of $5.23 per vote makes sense too. The Greens more social leaning policies such as their Wealth Tax, or previously attempted Capital Gains Tax means that their voter base is a lot more defined rather than the more centrist parties such as National or Labor. It would cost them more to convince those voters who are to the right of them to vote for Green.\nLooking at the demographics of Green supporters, their base is overwhelmingly concentrated in the younger generations (Gen Z, Millenials) and are likely not to own their own home. I expect that Greens will be a significant voice in pushing for change around the housing crisis.\n4. ACT #Wow! ACT came back with a vengeance coming in with nine extra seats than they had the previous elections. I am genuinely quite impressed at how effectively ACT has been in stealing votes from National.\nHaving only one seat in parliament last year, they managed to accomplish something significant - introducing the End of Life Choice Act.\nACT has a similar scenario to the Greens where their dedicated voter base is already defined. However, unlike the Greens, the majority of their voters are older and own their own homes. Interestingly enough, this wealthier voter base did not translate to an increase in how much donations they received. But they did better than the likes of Greens who had double the amount of donations.\nMaybe it\u0026rsquo;s easier to convince \u0026lsquo;gun enthusiasts\u0026rsquo; to vote for them where there is no other party who are realistically offering gun law repeals.\nI\u0026rsquo;m interested to see how ACT\u0026rsquo;s presence in our political environment will impact National in the long term. Will National continue to shift itself right to try to recapture these lost voters?\nMethodology \u0026amp; Assumptions #All sources are openly available from Stats New Zealand. If you want to do something similar, I highly encourage you to do so.\nMethodology # Meshblocks geometries were geometrically unioned by it\u0026rsquo;s \u0026lsquo;Statistical Area 2\u0026rsquo; (SA2) grouping. Preliminary voting locations were geolocated and then intersected with the prepared geometry above. Total votes were summed using party name, and SA2 area name. The top ten SA2 areas for each major party was chosen as a representative sample for its voter base. The metric chosen was the total amount of party votes as a percentage of total SA2 votes. The above excerpt shows this visually:\nRed is the unioned meshblock geographies grouped by SA2 area Blue represents the underlying meshblocks Yellow represents the geolocated location of a voting station Assumptions \u0026amp; Limitations # Preliminary votes have been used, this is subject to change as numbers are finalised. I\u0026rsquo;ve excluded postal votes, as well as \u0026rsquo;non-ordinary\u0026rsquo; voting places such as rest homes or prisons. I excluded these as I was not able to locate these votes to an SA2 area accurately. I\u0026rsquo;ve assumed that the voting stations represent the SA2 area which they overlap. Unfortunately, there are cases which SA2 areas do not entirely overlap Electorate areas, meaning some places would be split. To avoid any complications or misrepresentations which may arise from this, I opted to keep it simple. ","date":"21 October 2020","permalink":"/posts/election-results-advanced-analytics/","section":"Posts","summary":"\u003cp\u003eI\u0026rsquo;ll be the first to admit that I\u0026rsquo;m not the greatest fan of politics, in a typical scenario I\u0026rsquo;d avoid talking about it like the plague. However, as the elections have come and gone, it\u0026rsquo;d be sacrilege for me to pretend the goldmine of electoral data now available did not exist. In this case, my love for all things data outweighs my disdain for politics. So I thought, why not make it fun?\u003c/p\u003e","title":"Breaking Down the 2020 Elections"},{"content":"New Zealand has officially entered into a recession after 11 years following two consecutive periods of negative GDP growth. This last time New Zealand was in a recession was in 2008 due to the global financial crisis. Given the current circumstances, it\u0026rsquo;s no surprise that this has occurred.\nGiven the proximity to the elections, it\u0026rsquo;s also no surprise that many political parties have taken the opportunity to blast the incumbent government for their \u0026lsquo;poor handling\u0026rsquo; of our COVID-19 response. I had a look at our recovery so far in my previous post.\nIn my opinion, many news sources and political parties have significantly mis-contextualized the extent of how significant our recession has been.\nHeadline Numbers # Recession, noun, a period of temporary economic decline during which trade and industrial activity are reduced, generally identified by a fall in GDP in two successive quarters.\nThis visualization is something you may have seen frequently on the mainstream news sites. It highlights and shows our drastic fall into a recession. Without context, you\u0026rsquo;d be correct to be incredibly alarmed. U\nUnlike other countries, New Zealand could quickly return to relative normalcy, so why are we still entering a recession?\nTourism and GDP #On 19 March 2020, New Zealand closed its borders to almost all travelers, this in conjunction with our effective lockdown procedures, New Zealand got COVID-19 under control very quickly.\nThis border closure had an immediate effect, with our monthly arrivals plummeting to practically zero by the start of April. This sharp drop in overseas visitors significantly impacted New Zealand\u0026rsquo;s tourism industry, which makes up a significant proportion of its economy.\nIn FY19 international and domestic tourism contributed $40.9B of expenditure into our economy; international tourism equaled $17.162B. The indirect and direct value of tourism was quantified to be a 9.8% contribution to our GDP.\nContextualizing the Numbers #Let\u0026rsquo;s contextualize the GDP graph from our \u0026lsquo;Headline Numbers\u0026rsquo; section with the above in mind.\nIf we convert the graph from percentage to nominal terms, we can see that New Zealand GDP contracted by $922M and $7.685B for quarters one and two, respectively, this is a total of $8.067B in lost GDP.\nAt the time of writing, it has been 182 days since New Zealand closed its borders. We can pro-rata the value of FY19\u0026rsquo;s total international tourism expenditure as our proxy for what we\u0026rsquo;ve lost by international tourism in FY20. This figure is $8.557B in lost expenditure.\nI believe that our borders\u0026rsquo; closure was an essential step in getting our domestic economy to recover at the pace it has. The above graph shows that the recession New Zealand has entered was practically unavoidable.\nGiven our significant reliance on tourism, closing our borders would lead to the inevitable loss of foreign tourism.\nConclusion #It\u0026rsquo;s disappointing to see so many places do such shallow analysis of New Zealand\u0026rsquo;s recession. Whether intentionally or negligently, reporting NEW ZEALAND IN RECESSION without doing the bare minimum to explain the context will lead to unnecessary panic in a time where it is certainly not needed.\nThis post is not aiming to downplay the significance of our country entering into a recession, but more to provide a level headed and pragmatic view of it considering extenuating circumstances at a macro level.\nLastly, I can\u0026rsquo;t imagine a scenario where New Zealand does not enter into a recession during a global pandemic without seriously sacrificing its performance in managing cases. I would take the option where New Zealanders don\u0026rsquo;t die en masse over one where we all make slightly more money.\n","date":"17 September 2020","permalink":"/posts/nz-negative-growth-unbiased-analysis/","section":"Posts","summary":"\u003cp\u003eNew Zealand has officially entered into a recession after 11 years following two consecutive periods of negative GDP growth. This last time New Zealand was in a recession was in 2008 due to the global financial crisis. Given the current circumstances, it\u0026rsquo;s no surprise that this has occurred.\u003c/p\u003e","title":"Diving Deeper Into New Zealand's Official Recession"},{"content":"","date":null,"permalink":"/tags/apis/","section":"Tags","summary":"","title":"APIs"},{"content":"","date":null,"permalink":"/tags/developer-tools/","section":"Tags","summary":"","title":"Developer Tools"},{"content":"I was working on an NBA related project the other week and I had decided I wanted to create my own API wrapper (for fun) as part of the project. The primary source of data I was looking to use was the stats.nba.com API, our biggest challenge in using this endpoint is the fact they do not want to use it.\nLet\u0026rsquo;s breakdown the issues we face:\nThey block calls of any suspicious activity very liberally The API is undocumented The API constantly changes I\u0026rsquo;ve done different projects on this API in the past with has aimed to address some of these challenges such as aionba, which incorporated asynchronous calls with rotating proxies and caching to avoid the chances of being blocked. This project was ultimately unfinished as it ended up being more reliable just to call the API slowly.\nThis post will talk about the second challenge, the fact that the API is undocumented. Whilst you could rely on other people\u0026rsquo;s lists that they have compiled, it\u0026rsquo;s often nice to be able to this on your own. I\u0026rsquo;ll breakdown the method I came up with this particular instance as it was a snap to do, and gave me a pretty comprehensive list to use for development purposes.\nIf you came here solely for the endpoints, you can find it here.\nTooling #For this method, you\u0026rsquo;ll need some basic familiarity with working with JSON data. We\u0026rsquo;ll be taking advantage of HAR data collected by Firefox (or similar) and using Python (you can use what you\u0026rsquo;re most comfortable with) to unpack this data and generate markdown documentation for our project.\nI use Firefox as my primary browser, however, I assume whatever browser you use will suffice provided it is a modern one.\nJavaScript or C# would probably work fine here too, in general, I use Python for quick and dirty projects where speed/performance is not a priority.\nMethod #There is a manual element in this process, and without significant effort to automate it, it\u0026rsquo;s probably unavoidable. If you intend to do this regularly to make sure your documentation is up to date, you could do this portion with macros or something like Selenium. In this case, I\u0026rsquo;ll be doing this as a one-off so I\u0026rsquo;ve done it manually.\nCollecting HAR Data # Note: Make sure you tick \u0026lsquo;Persist Logs\u0026rsquo;.\nFor this step, you\u0026rsquo;ll want to systematically go through every visible page on the available to you, and apply each filter possible. The purpose of this is that the code you\u0026rsquo;ll write will summarise this data into \u0026lsquo;valid\u0026rsquo; parameters and options for the various endpoints. This part is very tedious but it does not take that long.\nYou\u0026rsquo;ll notice very quickly that stats.nba.com throttles the majority of your requests. This highlights how badly they don\u0026rsquo;t want people using their data, to the point where they hamstring their user experience.\nOnce you\u0026rsquo;ve gone through the different pages and filters, simply save your HAR data from the network tab of your developer tools.\nAnalysing the HAR Data #This step is the interesting part, as you\u0026rsquo;ll be able to quickly see commonalities in the endpoint and how it\u0026rsquo;s used. In particular, common properties and their payloads are evident.\nIn the below example, you\u0026rsquo;ll note that I\u0026rsquo;ve specified predefined values of which I know their values. This is due to my laziness in wanting to minimize the amount of manual work in the previous step - as I know these values are essentially static, I did not bother changing their filter values.\nimport json import os from collections import defaultdict from urllib.parse import urlparse, parse_qs import strconv urls = [] folder = \u0026#39;har/\u0026#39; base_url = \u0026#34;https://stats.nba.com/stats/\u0026#34; for filepath in os.listdir(folder): f = open(folder + filepath) data = json.load(f) data = data.get(\u0026#39;log\u0026#39;) for entry in data[\u0026#39;entries\u0026#39;]: urls.append(entry[\u0026#39;request\u0026#39;][\u0026#39;url\u0026#39;]) endpoint_map = defaultdict(lambda: defaultdict(list)) predefined = { \u0026#34;LastNGames\u0026#34;: set([8, 11, 2, 6, 5, 7, 12, 14, 10, 3, 1, 9, 4, 13]), \u0026#34;Month\u0026#34;: set([8, 11, 2, 6, 5, 7, 12, 10, 1, 9, 4]), \u0026#34;SeasonSegment\u0026#34;: set([\u0026#39;Post All-Star\u0026#39;, \u0026#39;Pre All-Star\u0026#39;]), \u0026#34;SeasonType\u0026#34;: set([\u0026#39;Regular Season\u0026#39;, \u0026#39;All Star\u0026#39;, \u0026#39;Playoffs\u0026#39;]) } for url in urls: if \u0026#34;?\u0026#34; in url: endpoint = url.split(\u0026#39;?\u0026#39;)[0] endpoint = endpoint.split(base_url)[-1] params = parse_qs(urlparse(url).query) for key, value in params.items(): endpoint_map[endpoint][key] += value for endpoint in endpoint_map: for key, value in endpoint_map[endpoint].items(): if key in predefined: endpoint_map[endpoint][key] = predefined[key] else: endpoint_map[endpoint][key] = set(value) The steps above breakdown as follows:\nLoad the HAR file as a JSON. Grab the relevant data within the logs. Use a basic for loop to tidy the data and summarise it. Making our Documentation #Once our data is nicely organised and structured, we can quickly make some nice documentation.\nf = open(\u0026#39;output.md\u0026#39;, \u0026#34;w\u0026#34;) f.write(\u0026#39;# stats.nba.com Endpoints\\n\u0026#39;) f.write(f\u0026#39;This document was autogenerated on `{ str(datetime.datetime.utcnow()) }`, the below endpoints are valid to the best of my knowledge at time of creation. \\n \\n This document was prepared for educational purposes only, it should not be used for commercial use. \\n \\n __Note__: type inferences may not be correct, use this as a reference rather than a guide. \\n \\n\u0026#39;) for endpoint, params in endpoint_map.items(): f.write(f\u0026#39;\\n## `{endpoint}`\\n\\n\u0026#39;) f.write(f\u0026#39;### Parameters \\n\\n\u0026#39;) for key, values in params.items(): inferred_type = None if len(values) \u0026gt; 0: _ = [i for i in list(values)] inferred_type = set([strconv.infer(i) if strconv.infer(i) else \u0026#39;str\u0026#39; for i in _]) inferred_type = \u0026#34; ,\u0026#34;.join([f\u0026#39;`{i}`\u0026#39; for i in inferred_type]) f.write(f\u0026#39;* __{key}__:\\n\u0026#39;) f.write(f\u0026#39; * Inferred Type: { inferred_type } \\n\u0026#39;) f.write(f\u0026#39; * Valid Values: { \u0026#34;, \u0026#34;.join([f\u0026#34;`{v}`\u0026#34; for v in values]) }\\n\u0026#39;) The above code snippet writes each line of our data into a md document. Easy.\nYou can see the results here.\nClosing Thoughts #Next time you\u0026rsquo;re wanting to work with a non-public friendly API, give this method a try. I\u0026rsquo;ll likely follow up on this post in the future as to my approach to making an API wrapper and how I would automate the process done in this article.\n","date":"13 September 2020","permalink":"/posts/mapping-an-undocumented-api/","section":"Posts","summary":"\u003cp\u003eI was working on an NBA related project the other week and I had decided I wanted to create my own API wrapper (for fun) as part of the project. The primary source of data I was looking to use was the \u003ca href=\"stats.nba.com\"\u003estats.nba.com\u003c/a\u003e API, our biggest challenge in using this endpoint is the fact they do not want to use it.\u003c/p\u003e","title":"Mapping an Undocumented API Easily With Python"},{"content":"","date":null,"permalink":"/tags/reverse-engineering/","section":"Tags","summary":"","title":"Reverse Engineering"},{"content":"","date":null,"permalink":"/tags/economy/","section":"Tags","summary":"","title":"Economy"},{"content":"At the beginning of March, I made the unfortunate decision of investing into two exchange-traded funds (ETFs) which were SmartShare\u0026rsquo;s FNZ (NZ Top 50) and USF (Fortune 500). It was unfortunate because the world decided to set itself on fire due to the COVID-19 pandemic.\nDisclaimer: This post has comments regarding the personal investments I\u0026rsquo;ve made. This is absolutely not financial advice. If you have any questions regarding your personal circumstances, go talk to a financial advisor.\nI purposefully made the decision that I would not look at my ill-timed investments. I did not want the temptation of \u0026lsquo;panic selling\u0026rsquo; and I wanted to save myself the pain of seeing my hard-earned dollars disappear overnight. I never had any doubts that my funds would return to their starting position, it was more of a question of when it would return to their starting position.\nFund Performance as a Proxy #Recently, I was reviewing how my funds had performed over the last 6 months. It had occured to me that my two investments in FNZ and USF could serve as a proxy or an indicator of how well New Zealand has faired during the COVID-19 pandemic, especially given it\u0026rsquo;s strict and rigid lockdown measures.\nGiven that I had equally split my investment in two different funds representing the \u0026rsquo;top\u0026rsquo; of both New Zealand and the USA, it seemed like a good way to compare how detrimentally COVID-19 has impacted each economy.\nI acknowledge that this proxy has a very glaring and obvious bias which will skew the results. The bias being that COVID-19 is more likely to significantly impact smaller businesses and businesses that do not have the scale or means to absorb the financial burdens of this pandemic. Additionally, I\u0026rsquo;m cognizant some industries will be significantly more impacted than others, i.e. tourism versus agriculture.\nThe above diagram is the performance of both my FNZ and USF funds, indexed to a dollar-based on the first day of my investment (4 March 2020).\nOn 23 March 2020, the Prime Minister of New Zealand, Jacinda Ardern, announced that New Zealand would be going into a \u0026lsquo;Level 4\u0026rsquo; lockdown from the 25th March 2020 for four weeks. This meant that all non-essential businesses and travel had to be ceased.\nThe effect of such an announcement naturally had a dramatic and significant impact on the FNZ, the dollar I had invested 19 days prior had plummeted by 31%. On 27 April 2020, New Zealand lifted its \u0026lsquo;Level 4\u0026rsquo; lockdown which meant normalcy was returning, and by 8 June 2020, normalcy had returned in the form of a \u0026lsquo;Level 1\u0026rsquo; lockdown.\nNew Zealand has handled this pandemic with great care, and have done a good job minimizing the devastating impacts it may have had. Even compared to countries with much more lax lockdown measures, the NZF rebounded at almost the same rate as the USF.\nAdditionally, due to confidence gained in how the first breakout was handled and the results gained, a second \u0026lsquo;Level 3\u0026rsquo; lockdown occurring on the 11 August 2020 did not nearly have the same impact as the first lockdown. There was a \u0026lsquo;blip\u0026rsquo; before it bounced back up a few days later.\nThe Performance That Matters #It would be foolish to look solely at economic impact alone when considering how well a country has handled a pandemic. Unlike some other countries, New Zealand had made stamping out the virus early a key priority.\nThe results of these actions are evident in the graph below.\nThere is a stark difference in the two countries which had seemingly very similar economic impacts. The difference is that at the time of writing New Zealand has a total of 1,772 cases and 24 deaths, while the USA 6.26 million cases and 188 thousand deaths.\nWhilst New Zealand has had some significant advantages in dealing with this pandemic, such as being an island nation, it does not detract from the cohesiveness and solidarity in which the country (on a whole) has dealt with COVID-19. With talks of a vaccine becoming increasingly more fervent, it\u0026rsquo;s becoming more likely that New Zealand will come out the other end of this pandemic significantly better off than other less fortunate countries.\nIt\u0026rsquo;s easy to get fall into the doom-and-gloom and sensationalist headlines, but it\u0026rsquo;s important to step back and recognise our achievement to date - I think New Zealand has absolutely nailed its COVID-19 response.\n","date":"6 September 2020","permalink":"/posts/nz-usa-covid19/","section":"Posts","summary":"\u003cp\u003eAt the beginning of March, I made the unfortunate decision of investing into two exchange-traded funds (ETFs) which were SmartShare\u0026rsquo;s FNZ (NZ Top 50) and USF (Fortune 500). It was unfortunate because the world decided to set itself on fire due to the COVID-19 pandemic.\u003c/p\u003e","title":"How Well Has New Zealand Handled COVID-19?"},{"content":"","date":null,"permalink":"/tags/new-zealand/","section":"Tags","summary":"","title":"New Zealand"}]